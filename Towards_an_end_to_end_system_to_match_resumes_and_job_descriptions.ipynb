{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Towards an end to end system to match resumes and job descriptions.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA-gQKizq_6v",
        "colab_type": "code",
        "outputId": "d06fd91f-ff75-4a51-9267-0ea56fdd46ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install pdfminer.six"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/04/f62d5834c2bdf90afcaeb23bb5241033c44e27000de64ad8472253daa4a8/pdfminer.six-20200402-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 7.3MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/16/da16a22d47bac9bf9db39f3b9af74e8eeed8855c0df96be20b580ef92fff/pycryptodome-3.9.7-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 288kB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.1.0)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Installing collected packages: pycryptodome, pdfminer.six\n",
            "Successfully installed pdfminer.six-20200402 pycryptodome-3.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3LCzqEWVtSK",
        "colab_type": "code",
        "outputId": "0897cbac-2407-4e8e-b8d9-54c5c0c1f4ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install word2number"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting word2number\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=f90e8af0a79f3e17e1dc6cbc665e98efe87139a30a2e34852e0786263a92e123\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HezG324_zcCd",
        "colab_type": "code",
        "outputId": "af9913ca-cd36-4366-a23e-c8a2e1c1f63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=35559560ec050dd52c6b8b4dc1a9ef87dc38075959999987d48ba3c669cca0f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2dar1pf9/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4lhNkWV2l3",
        "colab_type": "code",
        "outputId": "fafc2065-0a31-4b7d-bd9e-9735d86c80d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 9.3MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkzOEUUrJPD7",
        "colab_type": "code",
        "outputId": "53fc6f3e-fe14-492f-d837-ec07b9bd60b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/AdrianHerczeg/NLP_job_matching"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP_job_matching'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1zYkeUXJTRK",
        "colab_type": "text"
      },
      "source": [
        "#Towards an end to end system for ranking resumes acording to job descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL2TAt1tQOTm",
        "colab_type": "text"
      },
      "source": [
        "##Abstract\n",
        "\n",
        ">The task of resume ranking has proven to be one of the most difficult ones for evey company, escalating in difficulty with the expansion of the company and the number of interested recruiters about a specific job.\n",
        "For small companies, hiring specific personal can be difficult given the reduced income, on the other hand, having hundreds or even thousands of resumes sent to one specific job such being the case of global companies like Google, Facebook and many others, the costs of dedicated personel increases very much.\n",
        "In any of the above mentioned cases the dedicated persons that evaluates resumes wastes a lot of time and effort, witch could be directed to more vital tasks in the company.\n",
        "A substantial effort has been dedicated to solve this problem from the research comunity by automating the process of ranking resumes, however, the task of automating the information extraction process from unstructured text has proven to be one of the most difficult ones. The current sollutions covers a wide variety of domains and technologies, however, even to this day a perfect solution could not be discovered. In this paper we present our sollution to the problem of matching resumes and job descriptions in order to accurately rank them, by using text preprocessing and a rule based aproach. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu7RqJg-iGQd",
        "colab_type": "text"
      },
      "source": [
        "##1.Introduction\n",
        "\n",
        "> With the ever-shifting online ecosystem and the constant evolution of it on many diffent branches and domanins, two common and universally met goals were the automation and ease of human tasks. One domain in witch the evolution was strongly felt was the recruitment process in many companies. The process evolved from the early stages in witch the recruiters would send there resumes to the specific companies, at witch one or more dedicated employees would check the resumes to decide whether or not they could fit the specific job requirements. Later, with the appearance of social network such as LinkedIn and job portals such as ZipRecruiter, Upwork, LinkUp and many others, the interaction between recruiters and companies became much more accessible. In this stage(our current stage) people can post their resumes on a specific job portal on witch job they are interesed in, and companies can collect those resumes, analyze them and decide witch candidate is best suited for the job. The main dizadvantage of the current recruitment process is the necesity of human intervention in the resume analyzing process. The task of resume analyzing is, for most persons, very easy, very rare it happens for someone to misinterpret or not understand information from a resume. However, even if the task is easy for a human being, it still requires time to invest in order to go through the resume. This time is multiplied by the number of resumes, increasing as either the companies brand extends or more recruiters become interested in a specific job, being necesary to alocate specific persons(sometimes entire teams) and resources only to the task of reading and ranking the resumes, persons and resources that could most likely be better used in other places. To solve this problem, the solution of automating the information extraction process has been explored by a large number of researchers, however, even to this day the perfect solution has not yet been found. The task of information extraction from unstructured text has proven to be one of the most difficult tasks for a machine, even if for a human being is relatively simple. Even if the perfect solution has not yet been found, the recent progress made in the domain of natural language processing, gives us with a variety of tools that can help machines in better understanding unstrucutred text. From a structural perspective, the resume can be seen as a semistructured document, since it has some common sections(personal information, work experience, education, skills etc.). However, given the evolution of resumes and the vast variety of formats they are coming in, the argument can be made that it can be seen as an unstructured document, even more from the perspective of a machine. The job descriptions on the other hand can be seen completly as unstrucutred texts, even if they have very small common points sometimes, such as skills, education, experience. Given the unstructured nature of these two documents, some very important questions in designing a system that uses information from these has to be asked, questions like: What is the porpose of the designed system? What do we intend to do with the extracted information? Depending on the asnwers, the aproach regarding the information extraction process can be diffrent from system to system. For example, if a company designs a system to automate interaction with all their employees to send them news regarding a specific event in that company, the system can extract the personal information from their resumes, their skills in this case being less important. On the other side, if a company needs to rank the resumes based on some specific criteria, for example their suitability for a specific job description, the information that should be extracted are skills and education, their personal details being irrelevant in the ranking process. In this paper we present our aproach for the latter, our vision for a system that is capable of ranking resumes based on their suitability for a specific job description. The system uses in the first step a preprocessing pipeline to prepares the text, then splits all te resumes into pairs of two, one resume and the job description on each pair. The main principle of this aproach is to calculate the similarity score between the resume and the job description and rank the resumes based on this score. To do this, we then use the tf-idf metric to determine the weight of each term in the resume and the job description. Then we apply an entity recognition process, searching for named entities in the job description, and keeping only their score and their coresponding score in the resume. The last step is applying the cosine similarity on the scores of the extracted entities, giving us the ranking score. The rest of the paper is strucuted as follows: \n",
        "* Section 2 presents the related works\n",
        "* Section 3 presents the proposed aproach in detail\n",
        "* Section 4 presents the dataset and the results obtained by testing the proposed solution \n",
        "* Section 5 presents the conclusion and future work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndp3aVok4dH3",
        "colab_type": "text"
      },
      "source": [
        "##2.Related works\n",
        "\n",
        ">As stated earlier, the task of information extraction from unstructured and semistructured documents has been the focus a vast amount of researchers. A large number of sollutions ranged from machine learning alghoritms like support vector machine, conditional random field, k-NN, hiden markov models, to deep learning and usage of a large variety of neural networks to rule based aproaches that extract information based on predetermined rules. \n",
        "\n",
        "\n",
        "\n",
        ">Machine learning based aproaches proved to have promissing results in this task, H. Han [1] presented a machine learning based aproach for automated information extraction using the support vector machine classifier. Their aproach used a support vector machine based metadata extraction alghoritm with gaussian kernel to label the text in 15 predetermined classes. Their feature extraction process used word and line specific features to represent their data. Then the authors used line independent classification and contextual line classification to label every line of text with the labels of the words inside the line, and finally determined the boundaries of every label inside the line using information such as punctuation marks. Their aproach managed to score at least 80% accuracy on targeting every word, however some prior data was requiered. \n",
        "\n",
        "\n",
        ">The concepts presented in their article showed a lot of potential for SVM classifiers in this domain, and were studied further by more researchers. In the process of information extraction from resumes using SVM based aproaches, Jiaze Chen [2] presented a more specific solution, regarding the information extraction from resumes in PDF format. Their solution used a set of predetermined rules regarding the position and the layout of words to segment the block in one of seven predetermined categories. The next step was prepresented by labeling the blocks using a support vector machine classification alghoritm implemented using LIBSVM [3]. The feature vectors contained both content-based features and layout-based features. The final step in their aproach was the usage of conditional random fields to determine the attribute for each word from a predetermined set of labels. Their results are very wide, managing an F1 score from around 50% to 90% depending on the label. This aproach managed to classify the resume in the proposed labels, however some problems appeared, regarding the labeling of specific parts of the resume, like the persons name. The authors explained that, the reason is the position of the name outside of the personal block, hightlighting the importance of exact segmentation of the resume.\n",
        "\n",
        ">The information extraction process from resumes has also been aproached from an ontology perspective. Duygu Celik [4] presented and ontology based system, representing the data from the resume using OWL and RDFS. Then they segmented the resume in blocks with a specific label from a predetermined set of labels by using seven knowledge bases that contains seven diffrent ontologies. The ontologies refer to : education, locations, abbreviations, occupations, organizations, concepts, and resume. To match the terms from the resume with the concepts from the ontology knowledge bases, they proposed a semantic matching step to corectly eliminate the disambiguation of a specific word and use the Jaro-Winkler distance alghoritm to calculate the match score and chose the concept from with the best score. The aproach did not use any machine learning alghoritms, but managed to succesfully extract the information, however the amount of data requiered prior is enormous, six ontologies being necesary.\n",
        "\n",
        ">Another very common sollution is the rule based aproach. Jinesh Dhruv (https://github.com/jineshdhruv8/ResumeParser/blob/master/Resume_Parser.pdf) proposed a rule based aproach for resume parsing in PDF format. They propose the segmentation of the resume in six blocks by using a fuzzy string matching alghoritm to search for keywords. Once a keyword was found, the near words where added to the keyword and the structure was adnotated with the label of the respective keyword. They also use layout based features to better determine the boundaries of the segments. For the information extraction part they proposed a specific rule based aproach for each block, using NLTK as the implementation tool. The results showed poor performance in some cases, the reason being the lack of keywords the weak layout rules, however some features such as name, e-mail, phone where extracted with very good accuracy, scoring an F1-score of 91% proving that rule-based aproaches can have good results depending on the context.\n",
        "\n",
        ">The aproach of a standard resume format was also proposed. Pooja, et al [6] discussed the idea of a resume convertor that takes resumes in diffrent formats and converts them to a standard format. They proposed the parsing of each word form the resume to check for \"relevat words\" and the storage of these words in key-value pairs. Finally the authors proposed a reduction of key-value pairs to select only the relevant pairs and create a stadard resume based on these. The authors also explained the main aproaches used in the task of resume parsing and the disadvantages of current day job portals.\n",
        "\n",
        ">Another solution was proposed by Papiya Das [7]. The authors presented a rule based pipeline for extracting information using text preprocessing, word frequency counting method and keyword search alghoritms. They also explained their choice of  using specific inplementation tools flor NLP such as Python and R.\n",
        "\n",
        ">Khan Tabrez Mohd. Tahir(http://www.ijircce.com/upload/2016/april/218_Intelligent.pdf) also proposed a rule-based aproach for resume parsing, implementing a ranking system that ranks the candidates based on their skillset, experience and projects. The authors proposed a three steps system, the first step being the lexical analysis, in witch the resume is traversed from start to end in search of keywords based on a predetermined dictionary. The identified keywords will indicate the blocks, and use a series of diffrent entity recognizers to identify diffrent features for each block. The authors idetified in this step the same problem as identified in [2], witch is that the name of the person may appear on the top of the resume instead of the first block, witch will result in the wrong segmentation.The second step was represented by the syntactic alaysis to identify the synthactic form of the structure of the sentence. The last step was the semantic analysis witch helped in the identification of word structures with same meaning but diffrent forms (ex: \"University of ABC\" / \"ABC University\").\n",
        "\n",
        "\n",
        ">Another aproach for a resume ranking system was proposed by Vedant Bhatia [8].\n",
        "The authors used the LinkedIn resume format as standard and managed to succesfully create a parser based on document metadata heuristic rules that can convert resumes from any format in LinkedIn format with 97% accuracy. Then they convert the LinkedIn resumes to HTML, to extract metadata from the resume, and finally use the sequence-pair classification feature of BERT language representation model [9] to match the previous work experience of the candidates with the job description. The aproach performed admirably managing to score 72% accuracy, however previous work experience was requiered in the resume, making it difficult to adapt to recruiters without it.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qQVXyQugpTt",
        "colab_type": "text"
      },
      "source": [
        "##3.The proposed sollution\n",
        "\n",
        ">As stated in the introduction, the type of extracted information from resumes can be diffrent from task to task. The only purpose of the proposed system is to rank resumes based on job descriptions, therefore it will focus on details scuh as skills instead of personal information.\n",
        "\n",
        ">The sollutions can be seaparated in four main steps : \n",
        "1. Text preprocessing\n",
        "2. TF-IDF on a corpus level\n",
        "3. Entity recognition\n",
        "4. Score calculation. These four steps will be discussed in detail in this section, alongside every step of the implementation.\n",
        "\n",
        ">For the implementation, we use Python with a series of libraries designed for NLP, data manipulation and data visualization. \n",
        "\n",
        ">In the following paragraphs, every step of the process will be explained properly and the code for the implementation will be presented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LZUJ2FcDMNY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###A.Data reading\n",
        "\n",
        ">For data reading pdfminer.six proved to be the best package beacause of both simplicity to use and eficiency.As presented in [3], PDF proved to be a very powerfull format to store data, beacause it contains layout specific information, such as font-size and font-name.Also, the pdf format can contain metadata when done with proper tools, as mentioned in [8] witch can be very usefull. However beacause of all these features, the PDF format can be difficult to completly understand, this also being the case with the this library. Our experiments proved that some data such as the layout information can be difficult to extract regardless of the library, therefore we focus only on text data, witch can be extracted relatively easy. Below the implementation of the method for extracting text from PDF is presented in detail.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNJZ9so7rWPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracting text from pdf\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def pdf_to_text(path):\n",
        "    \"\"\"\n",
        "    Extracts the text from the PDF document located on the path given as the argument,\n",
        "    and returns the extracted text.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "    path : str\n",
        "      the path to the respective document\n",
        "    \"\"\"\n",
        "\n",
        "    manager = PDFResourceManager()\n",
        "    retstr = BytesIO()\n",
        "    layout = LAParams(all_texts=True)\n",
        "    device = TextConverter(manager, retstr, laparams=layout)\n",
        "    filepath = open(path, 'rb')\n",
        "    interpreter = PDFPageInterpreter(manager, device)\n",
        "\n",
        "    for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
        "        interpreter.process_page(page)\n",
        "\n",
        "    text = retstr.getvalue()\n",
        "\n",
        "    filepath.close()\n",
        "    device.close()\n",
        "    retstr.close()\n",
        "    return text.decode('utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IkM7iNfDAbv",
        "colab_type": "text"
      },
      "source": [
        "###B.Text preprocessing\n",
        "\n",
        ">As explained by Satyaki Sanyal [10], data preprocess is the first step in natural language processing. Most of the information that comes from the real world contains errors. The application of a good preprocessing step before further operations are performed proved to solve most of the problems. The text preprocessing methods are numerous, resuming from converting letters to lower cases, to identify the syntactic value of every word in the text. \n",
        "This method is applied on both the resumes and the job descriptions.\n",
        "Our method of text preprocess is inspired from (https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79), our aproach being a shorter version adapted to our case and consists of the following:\n",
        "1. converting to lower cases  \n",
        "2. removing accented characters\n",
        "3. converting numbers to words\n",
        "4. removing whitespaces\n",
        "5. tokenization\n",
        "6. lemmantization\n",
        "7. part of speach identification\n",
        "8. eliminating punctuations\n",
        "9. eliminating stop words\n",
        "10. eliminating special characters\n",
        "\n",
        ">The first four steps are performed in order to help us execute comparisons between words much easier when we search based on diffrent criteria. \n",
        ">The fifth step(tokenization) is performed in order to split the entire text in tokens, based on punctuations and special characters. This step helps in indentificating the boundaries and type of every word.\n",
        "\n",
        ">The lemmantization represents the process of altering the form of a specific word to a standard form(usually dictionary form), based on the syntactic meaning of the word in a sentence. The main diffrence why we chose the lemmantization instead of stemming, is that stemming chops the end of a word based on specific rules, sometimes altering the meaning of that word instead reducing it to the dictionary form(for example on the word \"carpets\" lemmantization will either keep the word this way or transform it into \"carpet\",\n",
        "where the stemming would transform it into \"car\", altering the meaning).\n",
        "\n",
        ">The part of speach identification is performed in order to find the nature of the specific token, telling us if a word represents a number, a symbol, a punctuation mark, a stop word etc. Based on the identified part of speach, we then apply the remaining three steps, witch are the elimination of punctuation marks, special characters and stop words.\n",
        "\n",
        ">The implementation of the pipeline is presented below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZksg5DrzUMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy    #for text preprocessing and entity extraction\n",
        "\n",
        "#Load the general model for the english language\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gHwtHodXlhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess text \n",
        "from unidecode import unidecode\n",
        "from word2number import w2n\n",
        "\n",
        "\n",
        "\n",
        "#converting to lower cases\n",
        "def convert_to_lower(text: str):\n",
        "    return (str(text)).lower()\n",
        "\n",
        "\n",
        "#removing accented characters\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode(text)\n",
        "    return text\n",
        "\n",
        "#converting numbers to words\n",
        "def convert_number_to_words(token):\n",
        "    try:\n",
        "        x = str(w2n.word_to_num(token))\n",
        "        return x\n",
        "    except ValueError:\n",
        "        return token\n",
        "\n",
        "#removing extra white spaces\n",
        "def remove_whitespaces(text):\n",
        "    text = text.strip()\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(resume):\n",
        "    resume = convert_to_lower(resume)\n",
        "\n",
        "    resume = remove_accented_chars(resume)\n",
        "\n",
        "    resume = remove_whitespaces(resume)\n",
        "\n",
        "    doc = nlp(resume)\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    for token in doc:                       #tokenization\n",
        "        add = True                \n",
        "        text = token.lemma_                 #lemmantization\n",
        "        if token.pos_ == 'NUM':\n",
        "            text = convert_number_to_words(text)\n",
        "            add = True\n",
        "        if token.pos_ == 'PUNCT':           #eliminating punctuations\n",
        "            add = False\n",
        "        if token.is_stop:                   #eliminating stop words\n",
        "            add = False\n",
        "        if token.pos_ == 'SYM':             #eliminating special characters\n",
        "            add = False\n",
        "        if add:\n",
        "            tokens.append(text)\n",
        "            add = True\n",
        "\n",
        "    parsed_resume = \" \"\n",
        "\n",
        "    return parsed_resume.join(tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqoeLojNUD3Y",
        "colab_type": "text"
      },
      "source": [
        "###C. TF-IDF on corpus level\n",
        "\n",
        ">As explayned earlier, the main point of the system is to rank resumes based on a similarity formula.To achive this goal we split the documents into small corpuses of two, one resume and their job description for. For each of these corpuses we calculate the TF-IDF value of every word in the preprocessed text.\n",
        "\n",
        ">As explained by VIJAYARANI, S. [11], the TF-IDF(term frequency-inverse document frequency) is a numerical statistic witch represents the importance of a word in a collection. The value of this numerical statistic increases with the number of appearances of a word in a specific set of documents (corpus in this case). \n",
        "\n",
        ">The TF-IDF metric proved to calculate the numerical weight of every word with very good results, increasing proportionaly with the number of appearances of the specific word, also reducing the weight of words that occur very\n",
        "frequently in the document set and increases the weight of terms that occur rarely.\n",
        "\n",
        ">To implement this feature, the best sollution proved to be SkiKit-learn.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380P5Enphw1M",
        "colab_type": "text"
      },
      "source": [
        "###D. Entity extraction\n",
        "\n",
        ">On this step we perforn an entity recognition process on every word on the job description. We serch for keywords that indicates specific skills based on a predefined dictionary. The dictionary is the same as (https://towardsdatascience.com/do-the-keywords-in-your-resume-aptly-represent-what-type-of-data-scientist-you-are-59134105ba0d), we chose this without any particular reason. The dictionary consists of keywords from 7 diffrent domains, everyone doamin having at least 10 words, some of them more. The domains and keywords can be seen below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCqlBy9KI45O",
        "colab_type": "code",
        "outputId": "135ece1b-cdca-4049-cfc5-a2b7f55f93dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "import pandas as pd     #for data visualization\n",
        "\n",
        "\n",
        "categories = pd.read_csv('/content/NLP_job_matching/Data/abilities.csv')\n",
        "categories"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Machine Learning</th>\n",
              "      <th>Deep Learning</th>\n",
              "      <th>R Language</th>\n",
              "      <th>Python Language</th>\n",
              "      <th>NLP</th>\n",
              "      <th>Data Engineering</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>statistical models</td>\n",
              "      <td>linear regression</td>\n",
              "      <td>neural network</td>\n",
              "      <td>ggplot</td>\n",
              "      <td>python</td>\n",
              "      <td>nlp</td>\n",
              "      <td>aws</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>statistical modeling</td>\n",
              "      <td>logistic regression</td>\n",
              "      <td>keras</td>\n",
              "      <td>shiny</td>\n",
              "      <td>flask</td>\n",
              "      <td>natural language processing</td>\n",
              "      <td>ec2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>probability</td>\n",
              "      <td>K means</td>\n",
              "      <td>theano</td>\n",
              "      <td>cran</td>\n",
              "      <td>django</td>\n",
              "      <td>topic modeling</td>\n",
              "      <td>amazon redshift</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>normal distribution</td>\n",
              "      <td>random forest</td>\n",
              "      <td>face detection</td>\n",
              "      <td>dplyr</td>\n",
              "      <td>pandas</td>\n",
              "      <td>lda</td>\n",
              "      <td>s3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>poisson distribution</td>\n",
              "      <td>xgboost</td>\n",
              "      <td>neural networls</td>\n",
              "      <td>tidyr</td>\n",
              "      <td>numpy</td>\n",
              "      <td>named entity recognition</td>\n",
              "      <td>docker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>survival models</td>\n",
              "      <td>svm</td>\n",
              "      <td>convolutional neural network(cnn)</td>\n",
              "      <td>lubricate</td>\n",
              "      <td>scikitlearn</td>\n",
              "      <td>pos tagging</td>\n",
              "      <td>kubernets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>hypothesis testing</td>\n",
              "      <td>naive bayes</td>\n",
              "      <td>recurent neural network(RNN)</td>\n",
              "      <td>knitr</td>\n",
              "      <td>sklearn</td>\n",
              "      <td>word2vec</td>\n",
              "      <td>scala</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bayesian inference</td>\n",
              "      <td>pca</td>\n",
              "      <td>object detection</td>\n",
              "      <td>NaN</td>\n",
              "      <td>matplotlib</td>\n",
              "      <td>word embedding</td>\n",
              "      <td>teradata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>factor analysis</td>\n",
              "      <td>decision tree</td>\n",
              "      <td>yolo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>scipy</td>\n",
              "      <td>lsi</td>\n",
              "      <td>google big query</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>forecasting</td>\n",
              "      <td>svd</td>\n",
              "      <td>gpu</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bokeh</td>\n",
              "      <td>spacy</td>\n",
              "      <td>aws lambda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>markov chain</td>\n",
              "      <td>ensemble models</td>\n",
              "      <td>cuda</td>\n",
              "      <td>NaN</td>\n",
              "      <td>statsmodel</td>\n",
              "      <td>genism</td>\n",
              "      <td>aws emr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>monte cario</td>\n",
              "      <td>boltzman machine</td>\n",
              "      <td>tensorflow</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nltk</td>\n",
              "      <td>hive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>lstm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nmf</td>\n",
              "      <td>hadoop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>doc2vec</td>\n",
              "      <td>sql</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>opencv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cbow</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bag of words</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>skip gram</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bert</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sentiment analysis</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chat bot</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Statistics  ...  Data Engineering\n",
              "0     statistical models  ...               aws\n",
              "1   statistical modeling  ...               ec2\n",
              "2            probability  ...   amazon redshift\n",
              "3    normal distribution  ...                s3\n",
              "4   poisson distribution  ...            docker\n",
              "5        survival models  ...         kubernets\n",
              "6     hypothesis testing  ...             scala\n",
              "7     bayesian inference  ...          teradata\n",
              "8        factor analysis  ...  google big query\n",
              "9            forecasting  ...        aws lambda\n",
              "10          markov chain  ...           aws emr\n",
              "11           monte cario  ...              hive\n",
              "12                   NaN  ...            hadoop\n",
              "13                   NaN  ...               sql\n",
              "14                   NaN  ...               NaN\n",
              "15                   NaN  ...               NaN\n",
              "16                   NaN  ...               NaN\n",
              "17                   NaN  ...               NaN\n",
              "18                   NaN  ...               NaN\n",
              "19                   NaN  ...               NaN\n",
              "\n",
              "[20 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csaoVL_ZjnZR",
        "colab_type": "text"
      },
      "source": [
        ">Based on the above presented keywords, we use a NLP alghoritm to search for their occurances in the job description and save the previous calculated TF-IDF of the indentified words from both the job-description and the resume.For the implementation we use Spacy's PhraseMatcher alghoritm to efficiently search for keywords in the preprocessed text, as can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwWnwvxf0BgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#skills\n",
        "from spacy.matcher import PhraseMatcher   #for entity extraction\n",
        "import numpy as np                        #for mathematical calculations\n",
        "\n",
        "\n",
        "\n",
        "def is_nan(x):\n",
        "    return x is np.nan or x != x\n",
        "\n",
        "\n",
        "\n",
        "def get_skills(resume):\n",
        "    skills = {}\n",
        "    doc = nlp(resume)\n",
        "    for category in categories:\n",
        "        matched_skills = []\n",
        "        terms = [term for term in categories[category]\n",
        "                 if is_nan(term) == False]\n",
        "        patterns = [nlp.make_doc(text) for text in terms]\n",
        "        matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "        matcher.add(\"TerminologyList\", None, *patterns)\n",
        "        matches = matcher(doc)\n",
        "\n",
        "        for match_id, start, end in matches:\n",
        "            span = doc[start:end]\n",
        "            matched_skills.append(str(span))\n",
        "        skills[category] = matched_skills\n",
        "    return skills\n",
        "\n",
        "\n",
        "\n",
        "def in_skills(text, job_description):\n",
        "    skills = get_skills(job_description)\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    for key in skills.keys():\n",
        "        if preprocessed_text in skills[key]:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eAnBFkwkxAq",
        "colab_type": "text"
      },
      "source": [
        "###E.Score calculation\n",
        "\n",
        ">This is the final step fo the matching alghoritm.In this step, we apply the cosine similarity formula to calculate the score.\n",
        "\n",
        ">Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. This measure proved to be very good in tasks like text classification, examples being the one of Faizan Javed [12]. In their aproach, the system uses the cosine distance (the compement of cosine similarity) as the distance metric for clustering alghoritms to label job descriptions. The performance of this measure was also tested by KADHIM [13], the outhors proving the importance of text preprocessing and the performance of cosine similarity in text classification with the TF-IDF metric.\n",
        "\n",
        ">We use the cosine similarity on the previously obtained vectors, witch contains the TF-IDF values of the extracted entities, calculating the matching score.\n",
        "\n",
        ">The implementation can be seen below, Skikit-learn being the best alternative for implementing the metric we could find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2EfiaHkdXoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#score calculation formula\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "#creating the vectors with the TH-IDF values\n",
        "def match_resume_job_description(job_description, resume):\n",
        "\n",
        "    preprocessed_resume = preprocess_text(resume)\n",
        "\n",
        "    preprocessed_job_description = preprocess_text(job_description)\n",
        "\n",
        "    corpus = [preprocessed_job_description, preprocessed_resume]\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    dense = X.todense()\n",
        "    denselist = dense.tolist()\n",
        "\n",
        "    df = pd.DataFrame(denselist, columns=feature_names,\n",
        "                      index=['Job Description', 'Resume'])\n",
        "\n",
        "    selected_skills = pd.DataFrame()\n",
        "\n",
        "    for category in categories.columns:\n",
        "        for term in categories[category]:\n",
        "            if not is_nan(term):\n",
        "                if in_skills(term, preprocessed_job_description):\n",
        "                    selected_skills = pd.concat(\n",
        "                        [selected_skills, df[term]], axis=1)\n",
        "\n",
        "    return selected_skills\n",
        "\n",
        "\n",
        "#calculating the score\n",
        "def calculate_match_score(dataframe_score: type(pd.DataFrame())):\n",
        "    \n",
        "    if len(dataframe_score.columns) != 0:\n",
        "        score = cosine_similarity(dataframe_score[0:1], dataframe_score)\n",
        "        return score\n",
        "    return [[0,0]]\n",
        "\n",
        "\n",
        "\n",
        "def match_and_calculate(job_description, resume):\n",
        "    matched_frame = match_resume_job_description(job_description, resume)\n",
        "    return (calculate_match_score(matched_frame))[0][1]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MijzGcgEtFnG",
        "colab_type": "text"
      },
      "source": [
        "##4.Dataset and experiments.\n",
        "\n",
        ">Our dataset is a sample of the (https://www.kaggle.com/dataturks/resume-entities-for-ner) dataset. It consists of the first 20 resumes, witch are associated to three job descriptions 6-7 resumes for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJ-e22ke5cb",
        "colab_type": "code",
        "outputId": "e15b7ada-5f99-4217-81dc-0f51a82daf58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import os               #used to manage the data\n",
        "\n",
        "path = os.path.join('NLP_job_matching','Data','Matches')\n",
        "dirs = os.listdir(path)\n",
        "dirs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Candidatul_ideal##03-04-2020  18-06-16.pdf',\n",
              " 'Job_Description_3##03-04-2020  18-11-36',\n",
              " 'We_are_looking_for_a_Big_Data_Engineer##03-04-2020  18-10-16.pdf',\n",
              " 'Candidatul_ideal##03-04-2020  18-06-16',\n",
              " 'We_are_looking_for_a_Big_Data_Engineer##03-04-2020  18-10-16',\n",
              " 'Job_Description_3##03-04-2020  18-11-36.pdf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7_LNlIpKvYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#storing the data in a dictionary for easier access\n",
        "documents = {}\n",
        "\n",
        "for dir in dirs:\n",
        "  if '.pdf' in dir:\n",
        " \n",
        "    job_description_path = os.path.join(path,dir)\n",
        "    job_description_text = pdf_to_text(job_description_path)\n",
        "    \n",
        "    job_description = {}\n",
        "    job_description['file_name'] = dir\n",
        "    job_description['text'] = job_description_text\n",
        "\n",
        "\n",
        "    resumes_folder = job_description_path.split('.pdf')[0]    \n",
        "    resumes_files = os.listdir(resumes_folder)\n",
        " \n",
        "    set_name = dir.split('.pdf')[0]+\"_SET\"\n",
        "\n",
        "    batch = {}\n",
        "    resumes = []\n",
        "    for resume_file in resumes_files:\n",
        "      resume = {}\n",
        "      resume['file_name'] = resume_file\n",
        "      resume['text'] = pdf_to_text(os.path.join(resumes_folder,resume_file))\n",
        "      resumes.append(resume)\n",
        "    \n",
        "    batch['job_description'] = job_description\n",
        "    batch['resumes'] = resumes  \n",
        "\n",
        "    documents[set_name] = batch  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dnRj5X4Tv9D",
        "colab_type": "code",
        "outputId": "f4ffc951-651b-471f-acd5-6a93b6dbd602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import json\n",
        "\n",
        "print(json.dumps(documents,indent = 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"Candidatul_ideal##03-04-2020  18-06-16_SET\": {\n",
            "        \"job_description\": {\n",
            "            \"file_name\": \"Candidatul_ideal##03-04-2020  18-06-16.pdf\",\n",
            "            \"text\": \"Candidatul ideal \\n\\nEducation: \\n\\ncomparable) \\n\\nKnow-How: \\n\\n\\u2022  Completed studies in a technical discipline (eg. Computer Science, Mathematics, Physics or \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\u2022  Comprehensive knowledge of data structures and algorithms. \\n\\n\\u2022  Understanding of the design and architecture of big data applications and proven experience with \\n\\nHadoop ecosystems (HDFS, Hive, Impala, YARN, Spark etc) \\n\\n\\u2022  Experience in working with cloud infrastructure (like Cloudera) and workflows for all new and existing \\n\\ndata sets ingested into data lake \\n\\n\\u2022  Capability to perform complex data transformations by means of efficient, performing and scalable \\n\\nprogramming in one of the following programming languages: Python / Java / C++/ C# \\n\\n\\u2022  Working experience with databases (SQL and noSQL) \\n\\nAdvantages if you have: \\n\\n\\u2022  Understanding of streaming platforms like Apache Kafka \\n\\n\\u2022  Software documentation, tracking (ex: Jira) and versioning tools \\n\\n\\u2022  Basic knowledge about testing frameworks like Selenium, Cucumber, JMeter or similar. \\n\\n\\u2022  Knowledge of virtualization technologies like Docker and Kubernetes. \\n\\n\\u2022  Passionate about data and big data technologies with an engineering mind set \\n\\n\\u2022  Motivation and commitment to get involved, learn and contribute to the development of leading edge \\n\\ntechnologies in collaboration with a global network of experts; \\n\\n\\u2022  Good communication, strong analytical skills, team oriented, initiative, perseverance and attention to \\n\\nPersonality \\n\\ndetails; \\n\\n\\u2022  Self-organized and able to estimate, prioritize and schedule own tasks. \\n\\n\\u2022  Target and quality orientation \\n\\nLanguages spoken: \\n\\n\\f\\u2022  Fluent in English; German can be an advantage; \\n\\nDescrierea jobului \\n\\n\\u2022  Contribute to the architecture and implementation of software products for systems that are \\n\\nintegrating and processing large data streams in (near-) real-time, in the context of various domains \\n\\nfrom Driver Assistance to Product Engineering or Manufacturing. \\n\\n\\u2022  Develop solutions for intelligent, scalable, reusable and reliable data processing (ETL) in Big Data \\n\\nframeworks \\n\\n\\u2022  Responsible for data modelling working on complex and varying data sources from different domains \\n\\nthat include everything from flat text files, semi-structured logs and messages, relational and non-\\n\\nrelational databases, sensor and event streams to images. \\n\\n#LikeABosch Benefits: \\n\\n\\u2022 25 days of annual leave, because work-life balance is essential to us; \\n\\nour Home Office program helps you do that; \\n\\n\\u2022 Lunch discounts and daily subsidies at our canteen/restaurant; \\n\\n\\u2022 Private medical insurance, because your health is a priority to us; \\n\\n\\u2022 Flexible working hours, but if you want to work 5 days per month from somewhere else, feel free, \\n\\n\\u2022 Flexible benefits - On top of your salary, we offer you a monthly budget via your benefit account, \\n\\n\\u2022 Internal development Trainings - we have local development programs for Project Managers, \\n\\nwhich can be used for several different services; \\n\\n\\u2022 Annual performance bonus; \\n\\nLine Managers and Experts; \\n\\n\\u2022 Inspiring working conditions; \\n\\n \\n\\n\\f\"\n",
            "        },\n",
            "        \"resumes\": [\n",
            "            {\n",
            "                \"file_name\": \"Resume4.pdf\",\n",
            "                \"text\": \"Ananya Chavan \\nlecturer - oracle tutorials \\n \\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Ananya- \\nChavan/738779ab71971a96 \\n \\nSeeking a responsible job with an opportunity for professional challenges \\nand utilize my skills \\nup to its extreme. \\n \\nWORK EXPERIENCE \\n \\nlecturer \\n \\nOracle tutorials -  Mumbai, Maharashtra - \\n \\nApril 2016 to Present \\n \\nfor computer science (STD 11th and 12th) (2 years) \\n\\u27a2 Worked at \\\"Dr.Babasaheb Ambedkar College, Chembur (W) \\\" as a lecturer \\nfor \\u2022 B.Sc. (Computer \\nScience & Information Technology) \\n\\u2022 F.Y.J.C. (Computer Science & I.T.) \\n\\u2022 S.Y.J.C. (Computer Science & I.T.) \\n\\u27a2 Worked at \\\"LIVE\\\" as a Head of the IT Department and Lecturer for Web \\ndesigning. \\n\\u27a2 Worked at \\\"Kohinoor College Of Hotel Management\\\" as visiting lecturer \\nfor SEM I. \\n\\u27a2 Working at \\\"ORACLE TUTORIALS\\\" as a lecturer for computer science (STD \\n11th and 12th) \\n \\nEDUCATION \\n \\nMCA \\n \\nMumbai University -  Mumbai, Maharashtra \\n \\nB.Sc. in Com.Sci \\n \\nMumbai University -  Mumbai, Maharashtra \\n \\nSKILLS \\n \\nSEARCH ENGINE MARKETING (2 years), SEM (2 years), ACCESS (Less than 1 \\nyear), AJAX (Less \\nthan 1 year), APACHE (Less than 1 year) \\n \\nADDITIONAL INFORMATION \\n \\nTechnical skills: \\nLanguages: C, C++, Java (J2EE), \\nWeb Component APIS:: Jdbc, Servlet, JSP. \\nFrameworks: Spring 4 & Struts 2 \\nORM Framework: Hibernate \\nWeb Development: Html5, CSS3, Java Script, Ajax &JQuery, Angular Js \\n \\nhttps://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-\\ndownload&ikw=download-top&co=IN \\n\\n\\fhttps://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\nApplication Servers: Apache Tomcat, \\nIDE: Eclipse, Netbeans \\nDatabase: Ms-Access, Mysql \\nOperating Systems: Windows 7, 8, 10 \\nFTP Client: Filezilla \\nVersioning Tools: Git \\n \\nProject Details: \\n \\n\\\"Real Estate Application\\\" (Client: Global Realtor PVT. LTD Pune) \\nFront-End: Java (J2EE), JDBC, Servlet, JSP, Jquery. \\nBack end: Mysql. \\nDuration: 6 Month (Internship) \\nCompany Name: AryanTech India Pvt. Ltd. Pune \\nMy Role: Developer as Trainee. \\nModule: Module 4. \\nDescription: Developed as a MCA Final SEM Project for \\n\\\"Global Realtors PVT.LTD, Hinjewadi, Pune.\\\" \\nThe Real Estate Web Application is an interactive, effective and revenue-\\ngenerating website \\ndesigned for the Real Estate Industry. The main objective of this \\napplication is to help the Real \\nEstate Company to display unlimited number of property listings on the \\nwebsite. \\n \\n\\\"Beauty Parlor Management System\\\" (B.Sc. (Com.Sci.)) \\nTool: VB 6.0 \\nLanguage: VB \\nDatabase: MS-Access \\nOperating System: Windows XP \\nThe Beauty Parlor Management System is an easy and effective system to \\nuse. The main features \\nof this system are to avoid manual work and keep storing all appointments \\nof customers. \\n \\n\\\"Web Designing Project (Reptiles.com) \\\" (B.Sc. (Com.Sci.)) \\nLanguage: HTML and ASP \\nTool: Dreamweaver 8.0 \\nDatabase: MS-Access \\nOperating System: Windows XP \\nThe Reptiles.com is a simple informative site. The main features of this \\nsystem are to give all \\ninformation of Snakes. \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume3.pdf\",\n",
            "                \"text\": \"Alok Khandai \\nOperational Analyst (SQL DBA) Engineer - UNISYS \\n \\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Alok-\\nKhandai/5be849e443b8f467 \\n \\n\\u2756 Having 3.5 Years of IT experience in SQL Database Administration, \\nSystem Analysis, Design, \\nDevelopment & Support of MS SQL Servers in Production, Development \\nenvironments & \\nReplication and Cluster Server Environments. \\n\\u2756 Working Experience with relational database such as SQL. \\n\\u2756 Experience in Installation, Configuration, Maintenance and \\nAdministration of SQL Server. \\n\\u2756 Experience in upgrading SQL Server. \\n\\u2756 Good experience with implementing DR solution, High Availability of \\ndatabase servers using \\nDatabase mirroring and replications and Log Shipping. \\n\\u2756 Experience in implementing SQL Server security and Object permissions \\nlike maintaining \\nDatabase authentication modes, creation of users, configuring permissions \\nand assigning roles \\nto users. \\n\\u2756 Experience in creating Jobs, Alerts, SQL Mail Agent \\n\\u2756 Experience in performing integrity checks. Methods include configuring \\nthe database \\nmaintenance plan wizard and DBCC utilities \\n\\u2756 Experience in using Performance Monitor, SQL Profiler and optimizing \\nthe queries, tracing long \\nrunning queries and deadlocks. \\n\\u2756 Experience in applying patches and service packs to keep the database \\nat current patch level. \\n\\u2756 Ability to manage own work and multitask to meet tight deadlines \\nwithout losing sight of \\npriorities.. \\n \\nWilling to relocate to: Bengaluru, Karnataka \\n \\nWORK EXPERIENCE \\n \\nOperational Analyst (SQL DBA) Engineer \\n \\nUNISYS -  Bengaluru, Karnataka - \\n \\nJuly 2016 to Present \\n \\n\\u2756 Having 3.5 Years of IT experience in SQL Database Administration, \\nSystem Analysis, Design, \\nDevelopment & Support of MS SQL Servers in Production, Development \\nenvironments & \\nReplication and Cluster Server Environments. \\n\\u2756 Working Experience with relational database such as SQL. \\n\\u2756 Experience in Installation, Configuration, Maintenance and \\nAdministration of SQL Server.  \\n\\u2756 Experience in upgrading SQL Server. \\n\\u2756 Good experience with implementing DR solution, High Availability of \\ndatabase servers using \\n\\n\\fDatabase mirroring and replications and Log Shipping. \\n\\u2756 Experience in implementing SQL Server security and Object permissions \\nlike maintaining \\nDatabase authentication modes, creation of users, configuring permissions \\nand assigning roles \\nto users. \\n \\nDBA Support Analyst \\n \\nMicrosoft Corporation -  Redmond, WA - \\n \\nhttps://www.indeed.com/r/Alok-Khandai/5be849e443b8f467?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\nJuly 2016 to Present \\n \\nClient Description: \\nMicrosoft Corporation is an American public multinational corporation \\nheadquartered in \\nRedmond, Washington, USA that develops, manufactures, licenses, and \\nsupports a wide range of \\nproducts and services predominantly related to computing through its \\nvarious product divisions. \\n \\nEnvironment: \\nMicrosoft has E2E development and production environment of more than \\n25000 servers and \\napplications. We are responsible for pro-active monitoring of all the \\nservers and their jobs using \\nmonitoring tools to reduce critical business impact by alerting \\nrespective peer teams. Microsoft \\nService Enterprise an ITSM tools are used for ticketing and SharePoint \\nportal is used to store all \\ntechnical and process documentation. \\n \\nRoles and Responsibilities: \\n\\u2022 Responsible for Database support, troubleshooting, planning and \\nmigration. Resource planning \\nand coordination for application migrations with project managers, \\napplication and web app \\nteams. Project involved guidance and adherence to standardized procedures \\nfor planned data \\ncenter consolidation for worldwide centers using in-house corporate and \\nthird party applications \\nbased on SQL 2000 in upgrade project to SQL 2005. \\n\\n\\u2022 Monitoring of database size and disk space in Production, Staging & \\nDevelopment environments \\n\\n\\u2022 Performed installation of SQL Enterprise 2005 64bit version on Windows \\n2003 servers on \\nEnterprise systems of clustered and standalone servers in enterprise Data \\nCenters. Patch \\napplications. \\n\\u2022 Failover cluster testing and resolution on HP servers as well as \\nmonitoring and backup reporting \\nsetup with Microsoft Operations Manager and backup teams. \\n\\n\\u2022 Working in Microsoft production environment which includes applications \\nand servers. \\n\\n\\f\\u2022 Configured Transactional Replication and Log Shipping with SQL Server \\nManagement Studio as \\nwell as basic account management and troubleshooting with connectivity, \\nsecurity and firewall \\nissues. \\n\\u2022 Handling issues related to Server Availability, Performance. \\n\\u2022 Performed Production support and on Call duties \\n\\u2022 Conducted Performance Tuning using SQL Profiler and Windows Performance \\nMonitor. \\n\\n\\u2022 Worked with various business groups while developing their \\napplications, assisting in database \\ndesign, installing SQL Server clients, phasing from development to QA and \\nto Production \\nenvironment. \\n \\nPrevious Project \\n\\u2756 Project Title: Finance Support \\n\\u2756 Client: Costco Wholesale Corporation (USA) \\n\\u2756 Team size: 22 \\n\\u2756 Role: DBA Support Analyst \\n\\u2756 Environment: Window 10 \\n \\n(SQL DBA Analyst) Engineer \\n \\nHCL Technologies -  Bengaluru, Karnataka - \\n \\nNovember 2014 to July 2016 \\n \\n\\u3013 Performed server installation and configurations for SQL Server 2005 \\nand SQL Server 2000. \\n \\n \\n \\n\\u3013 Performed installation of SQL Server Service Packs \\n\\u3013 Upgraded databases from SQL Server 2000 to SQL Server 2005. \\n\\u3013 Scheduled Full and Transactional log backups for the user created and \\nsystem databases in \\nthe production environment using the Database Maintenance Plan Wizard. \\n\\u3013 Setup backup and restoration jobs for development and QA environments \\n\\u3013 Created transactional replication for the reporting applications. \\n\\u3013 Implemented disaster recovery solution at the remote site for the \\nproduction databases using \\nLog Shipping. \\n\\u3013 Used System monitor to find the bottlenecks in CPU, Disk I/O and \\nmemory devices and \\nimproved the database server performance. \\n\\u3013 Used SQL Server Profiler to monitor and record database activities of \\nparticular users and \\napplications. \\n\\u3013 Used DBCC commands to troubleshoot issues related to database \\nconsistency \\n\\u3013 Worked with various business groups while developing their \\napplications, assisting in database \\ndesign, installing SQL Server clients, phasing from development to QA and \\nto Production \\nenvironment \\n \\n\\n\\fMicrosoft Corporation - \\n \\nNovember 2014 to July 2016 \\n \\nClient Description: \\n \\n\\u2756 Costco Wholesale Corporation operates an international chain of \\nmembership warehouses, \\nmainly under the \\\"Costco Wholesale\\\" name, that carry quality, brand name \\nmerchandise at \\nsubstantially lower prices than are typically found at conventional \\nwholesale or retail sources. The \\nwarehouses are designed to help small-to-medium-sized businesses reduce \\ncosts in purchasing \\nfor resale and for everyday business use. Individuals may also purchase \\nfor their personal needs. \\n \\n\\u2756 Responsibilities: \\n \\n\\u27a2 Performed server installation and configurations for SQL Server 2005 \\nand SQL Server 2000. \\n\\u27a2 Performed installation of SQL Server Service Packs \\n\\u27a2 Upgraded databases from SQL Server 2000 to SQL Server 2005. \\n\\u27a2 Scheduled Full and Transactional log backups for the user created and \\nsystem databases in \\nthe production environment using the Database Maintenance Plan Wizard. \\n\\u27a2 Setup backup and restoration jobs for development and QA environments \\n\\u27a2 Created transactional replication for the reporting applications. \\n\\u27a2 Implemented disaster recovery solution at the remote site for the \\nproduction databases using \\nLog Shipping. \\n\\u27a2 Used System monitor to find the bottlenecks in CPU, Disk I/O and \\nmemory devices and improved \\nthe database server performance. \\n\\u27a2 Used SQL Server Profiler to monitor and record database activities of \\nparticular users and \\napplications. \\n\\u27a2 Used DBCC commands to troubleshoot issues related to database \\nconsistency \\n\\u27a2 Worked with various business groups while developing their \\napplications, assisting in database \\ndesign, installing SQL Server clients, phasing from development to QA and \\nto Production \\nenvironment \\n \\n \\n \\nEDUCATION \\n \\nB.Tech in Computer Science and Engineering in CSE \\n \\nIndira Gandhi Institute Of Technology \\n \\n2012 \\n \\nSKILLS \\n \\nDatabase (3 years), SQL (3 years), Sql Dba \\n\\n\\f \\nADDITIONAL INFORMATION \\n \\nTECHNICAL PROFICIENCY \\n\\u2756 Operating Environment: [\\u2026] Windows95/98/XP/NT \\n\\u2756 Database Tool: SQL Management Studio (MSSQL), Business \\nDevelopment Studio, Visual studio 2005 \\n\\u2756 Database Language: SQL, PL/SQL \\n\\u2756 Ticket Tracking Tool: Service Now \\n\\u2756 Reporting Tools: MS Reporting Services, SAS \\n\\u2756 Languages: C, C++, PL/SQL \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume5.pdf\",\n",
            "                \"text\": \"Anvitha Rao \\nAutomation developer \\n \\n- Email me on Indeed: indeed.com/r/Anvitha-Rao/9d6acc68cc30c71c \\n \\nSeeking a software development internship position for Summer 2018 that \\nutilizes my technical \\nskills, education and passion for \\nsolving interesting problems as a software professional. \\n \\nWilling to relocate: Anywhere \\n \\nWORK EXPERIENCE \\n \\nAutomation developer \\n \\nSAP Labs -  Bengaluru, Karnataka - \\n \\nAugust 2016 to August 2017 \\n \\nWorked as an automation developer in the development of cloud based Human \\ncapital \\nmanagement System and \\nIntegration frameworks Dell Boomi and SAP HANA Cloud Platform \\nIntegration. \\n\\n\\u25cf Development of automation framework using Java Selenium and RestFul \\nAPI's for payroll \\nprojects. \\n\\u25cf Collaborating with customers on migrating Payroll projects to SAP HANA \\nCloud Platform \\nIntegration. \\n\\n\\u25cf Involved in regression, unity, functionality, performance, sanity and \\nacceptance test driven \\ndevelopment process. \\n \\nIntern \\n \\nSAP Labs -  Bengaluru, Karnataka - \\n \\nFebruary 2016 to July 2016 \\n \\nWorked on migrating Business intelligence tool, Universe Design Tool in \\nVisual Studio using C++. \\n\\n\\u25cf Automation of test cases for identifying operations supported by \\ndatabases using Java. \\n\\n\\u25cf Tools: Universe Design Tool, Information Design Tool, Webi and \\nPerforce. \\n \\nProjects: \\nGeoSpark Integrated with Apache Hadoop \\n\\n\\u25cf Identifies top 50 pick up points in New York city based on distributed \\nNew York taxi data using \\nApache Spark and Scala. \\nIaas Implementation: Cloud application for Image Recognition \\n\\u25cf This project uses Amazon S3, SQS and EC2 instances to provide Image \\nrecognition service to \\nusers by implementing load \\nbalancing and web services. \\nSpam Detection \\n\\n\\f\\u25cf Identifying spam messages using Natural Language processing techniques \\nin python. \\nAnalysis of Financial Data \\n \\nhttps://www.indeed.com/r/Anvitha-Rao/9d6acc68cc30c71c?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\n\\u25cf Exploratory data analysis of stock prices using various data \\nvisualization techniques in pandas \\nand d3.js \\nEffective prediction and prevention of air pollution caused due to \\nautomobiles using IoT and data \\nanalytics techniques \\n\\n\\u25cf Analyzes pollutants at different geographical locations and suggests a \\nleast polluted route on \\nan android application. \\n\\u25cf Co-authored a paper --Ref link: \\nhttp://www.ijrcct.org/index.php/ojs/article/view/1416 \\n \\nEDUCATION \\n \\nMasters in Computer Science \\n \\nArizona State University -  Tempe, AZ \\n \\nSeptember 2019 \\n \\nBachelor of Engineering in Computer Science \\n \\nM S Ramaiah Institute of Technology -  Bengaluru, Karnataka \\n \\nSeptember 2012 to June 2016 \\n \\nSKILLS \\n \\nJAVA (1 year), C++ (Less than 1 year), Hadoop (Less than 1 year), HADOOP \\n(Less than 1 year), \\nCSS (Less than 1 year) \\n \\nLINKS \\n \\nhttps://www.linkedin.com/in/anvitha-d-rao-65a068a7 \\n \\nADDITIONAL INFORMATION \\n \\nTechnical Skills: \\nProgramming Languages: C, C++, HTML/CSS, Java, Python, Javascript \\n \\nTechnologies: IoT, MySQL, PostgreSQL, D3js, Hadoop and Spark, Gephi \\n \\nhttps://www.linkedin.com/in/anvitha-d-rao-65a068a7 \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume0.pdf\",\n",
            "                \"text\": \"Abhishek Jha \\nApplication Development Associate - Accenture \\n \\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-\\nJha/10e7a8cb732bc43a \\n \\n\\n\\u2022 To work for an organization which provides me the opportunity to \\nimprove my skills \\nand knowledge for my individual and company's growth in best possible \\nways. \\n \\nWilling to relocate to: Bangalore, Karnataka \\n \\nWORK EXPERIENCE \\n \\nApplication Development Associate \\n \\nAccenture - \\n \\nNovember 2017 to Present \\n \\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft \\nQueries \\nfor the Bot which will be triggered based on given input. Also, Training \\nthe bot for different possible \\nutterances (Both positive and negative), which will be given as \\ninput by the user. \\n \\nEDUCATION \\n \\nB.E in Information science and engineering \\n \\nB.v.b college of engineering and technology -  Hubli, Karnataka \\n \\nAugust 2013 to June 2017 \\n \\n12th in Mathematics \\n \\nWoodbine modern school \\n \\nApril 2011 to March 2013 \\n \\n10th \\n \\nKendriya Vidyalaya \\n \\nApril 2001 to March 2011 \\n \\nSKILLS \\n \\nC (Less than 1 year), Database (Less than 1 year), Database Management \\n(Less than 1 year), \\nDatabase Management System (Less than 1 year), Java (Less than 1 year) \\n \\nADDITIONAL INFORMATION \\n \\nTechnical Skills \\n \\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-\\ndownload&ikw=download-top&co=IN \\n\\n\\f \\n \\n\\u2022 Programming language: C, C++, Java \\n\\u2022 Oracle PeopleSoft \\n\\u2022 Internet Of Things \\n\\u2022 Machine Learning \\n\\u2022 Database Management System \\n\\u2022 Computer Networks \\n\\u2022 Operating System worked on: Linux, Windows, Mac \\n \\nNon - Technical Skills \\n \\n\\u2022 Honest and Hard-Working \\n\\u2022 Tolerant and Flexible to Different Situations \\n\\u2022 Polite and Calm \\n\\u2022 Team-Player \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume2.pdf\",\n",
            "                \"text\": \"Akhil Yadav Polemaina \\nHyderabad, Telangana - Email me on Indeed: indeed.com/r/Akhil-Yadav-\\nPolemaina/ \\nf6931801c51c63b1 \\n \\n\\n\\u25cf Senior System Engineer at Infosys with 3.2 years of experience in \\nsoftware development and \\nMaintenance. \\n\\u25cf Maintained data processing using mainframe technology for multiple \\nfront end applications of \\nWalmart Retail Link platform and ensured on-time deliverables. \\n\\n\\u25cf Worked on automating the uses cases to reduce manual effort in solving \\nrepeating incidents \\nusing Service Now orchestration. \\n\\n\\u25cf Possess good analytical, logical ability and systematic approach to \\nproblem analysis, strong \\ndebugging and troubleshooting skills. \\n\\u25cf Good exposure to Retail domain. \\n \\nWilling to relocate to: hyderbad, Telangana \\n \\nWORK EXPERIENCE \\n \\nSenior Systems Engineer \\n \\nInfosys Limited -  Hyderabad, Telangana - \\n \\nJanuary 2015 to Present \\n \\n\\n\\u25cf Working on all the Major and Minor Enhancement requests as part of \\nMaintenance and Support \\nactivities \\n\\u25cf Identifying and fixing all the major defects in the applications, \\nperform root cause analysis for \\nproduction issues \\n\\n\\u25cf Being a subject matter expert, involved in multiple Knowledge transfer \\nand knowledge sharing \\nsessions with the client \\n\\n\\u25cf Leading a peer group and taking end to end responsibilities for all the \\ncritical issues/ \\nenhancements. \\n\\n\\u25cf Identifying the use cases which can be automated using Service Now \\nOrchestration \\n\\n\\u25cf Creating workflows to automate various tasks which involved manual \\nintervention \\n\\u25cf Direct interaction with the client on various business impacting issues \\non a daily basis \\n\\n\\u25cf Setting up Weekly Status Review meetings and Code Review meetings with \\nthe client \\n \\nSenior Systems Engineer \\n \\nInfosys Limited -  Hyderabad, Telangana - \\n \\nJanuary 2015 to Present \\n \\nTeam Size # 5 \\nProject Objective: \\nProviding end to end Maintenance and Support activity for data processing \\nof the most critical and \\n\\n\\fimportant Web portal 'Retail Link' along with over 40 applications used \\ndaily by all the Suppliers \\nand Business users of Walmart, the largest retailer in the world. Retail \\nlink is a portal which hosts \\n100's of applications developed across technologies for the suppliers \\nwhich help them to carry \\non day-to-day activities right from on boarding to tracking their sales. \\nThis involves supporting \\n \\nhttps://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-\\ndownload&ikw=download-top&co=IN \\nhttps://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\nvarious Decision Support System reports which helps the higher management \\nto take business \\ncritical decisions. \\n \\nResponsibilities: \\n\\n\\u25cf Working on all the Major and Minor Enhancement requests as part of \\nMaintenance and Support \\nactivities \\n\\n\\u25cf Identifying and fixing all the major defects in the applications, \\nperform root cause analysis for \\nproduction issues \\n\\n\\u25cf Being a subject matter expert, involved in multiple Knowledge transfer \\nand knowledge sharing \\nsessions with the client \\n\\n\\u25cf Leading a peer group and taking end to end responsibilities for all the \\ncritical issues/ \\nenhancements. \\n\\n\\u25cf Identifying the use cases which can be automated using Service Now \\nOrchestration \\n\\n\\u25cf Creating workflows to automate various tasks which involved manual \\nintervention \\n\\u25cf Direct interaction with the client on various business impacting issues \\non a daily basis \\n\\n\\u25cf Setting up Weekly Status Review meetings and Code Review meetings with \\nthe client \\n \\nEDUCATION \\n \\nElectrical and Electronics Engineering \\n \\nAnurag College of Engineering (Jntuh) \\n \\nSKILLS \\n \\nservicenow (1 year), Mainframe (3 years), cobol (3 years), Jcl (3 years), \\nTeradata (3 years) \\n \\nADDITIONAL INFORMATION \\n \\nTechnical Skills \\n\\u2022 Domain - Retail \\n\\u2022 Technology - Mainframe (COBOL, JCL, DB2, Teradata), Service now. \\n\\u2022 Operating System - Mainframe (z/OS) \\n\\u2022 Database - DB2, SQL, Teradata. \\n\\n\\f\\u2022 Utilities - FILE-AID, IDCAMS, DFSORT basics, LIBRARIAN, FTP/SFTP, CA-7 \\nbasics. \\n\\u2022 Tools - Query Management Tool (QMF), SQL Assistant, Service now, \\nRemedy. \\n \\nKey Strengths: \\n\\u25cf Effective Communication Skills and Zeal to learn. \\n\\u25cf Flexibility and Adaptability. \\n\\u25cf Good Leadership Qualities. \\n\\u25cf Analytical and Problem Solving Skills. \\n \\nAchievements: \\n\\n\\u25cf Received STAR award for working on various system improvement and \\nautomation activities \\n\\u25cf Received multiple INSTA awards for my performance in the projects \\nworked \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume6.pdf\",\n",
            "                \"text\": \"arjun ks \\nSenior Program coordinator - oracle India Limited \\n \\nBangalore City, Karnataka - Email me on Indeed: indeed.com/r/arjun-\\nks/8e9247624a5095b4 \\n \\nSeeking a position in a company where I can use my familiarity of the \\nfield and my educational \\nbackground for the profit of the company. My individuality as well as \\npast know-how should help \\nme in causative to the overall intensification of the company. \\n \\nSnaps \\n \\nExpertise, Certification and Training \\n \\n\\u2611 I am a Post Graduate in MBA with a total work experience of 6.8 years \\n\\u2611 3.3 years in IT. Currently working as a Senior Program coordinator. \\n3.5 years in BPO voice \\nsupport, \\n\\u2611 Good work ethics with excellent communication and interpersonal \\nskills. \\n\\u2611 Capable to delve into the new leading Technologies. \\n\\u2611 Pro active and self starter with the great ability of leadership. \\n\\u2611 Ability to work well in both a team environment and individually. \\n\\u2611 Able to handle multiple projects under tight deadlines. \\n\\u2611 Able to develop excellent rapport with peers, professionals and \\nmanagers. \\n \\n\\u2611 Trained on PMP (project management professional) Course \\n\\u2611 Diploma in Office Application {Ms-excel, Ms-word & Ms PowerPoint} \\n\\u2611 Diploma in Tally 9.0 \\n\\u2611 Milestone 2.0 from Infosys \\n\\u2611 Coach the coach from Infosys \\n\\u2611 Analytical ability from Infosys \\n\\u2611 Diffusion skills from Infosys \\n\\u2611 Personality development program from Bouyance \\n\\u2611 National entrepreneurship network training from NEN \\n \\nWilling to relocate to: Bangalore, Karnataka \\n \\nWORK EXPERIENCE \\n \\nsenior program coordinator \\n \\noracle -  Bengaluru, Karnataka - \\n \\n2014 to Present \\n \\nManaging Oracle\\u2019s LMS system and assist all processes supporting \\nEmployee/Partner trainings, \\nregistrations, and online publication, also provide support for Oracle's \\nLMS systems. \\n \\nProcess Specialist \\n \\nInfosys -  Bengaluru, Karnataka - \\n\\n\\f \\nhttps://www.indeed.com/r/arjun-ks/8e9247624a5095b4?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\nJuly 2011 to December 2014 \\n \\nResponsibility Areas: \\n\\u3013 Daily, Weekly and Monthly performance monitoring. \\n\\u3013 Quality monitoring of advisors and to identify area of improvement. \\n\\u3013 Customer satisfaction monitoring and providing feedback to improve \\ncustomer experience. \\n\\u3013 Facilitating interactions with various support functions such as HR, \\nCommand centre (work \\nforce management team), finance, and transport teams. \\n\\u3013 Preparing Balance Score Card for the associates. \\n\\u3013 Matching the contract of the client and rostering agents. \\n\\u3013 Manage sectors on a shift basis. \\n\\u3013 Seat utilization plan. (Physical Resource allocation) \\n\\u3013 Coaching and mentoring the team members to improve on productivity and \\naccuracy. \\n\\u3013 Holding Process review meetings with the senior management to review \\nperformance on an \\nongoing basis. \\n\\u3013 Imparting training to team members on regular basis. \\n\\u3013 Mentor new team leaders on the Floor. Has been a technical training \\nresource for all New Hires \\nincluding Team Leaders. \\n\\u3013 Have been conducting the Operations orientation programs for the new \\njoiners on the floor. \\n\\u3013 Manage escalated customer enquiries / complaints, Share best practices \\nacross the \\n\\u3013 Process & facilitate process improvements initiatives. \\n\\u3013 Monitoring and evaluating the existing processes, performance and SOPs \\nof each agent against \\ncommitted SLA\\u2019s. \\nImparting timely feedback and reporting to the India and UK leadership \\nteam on process. \\n \\nEDUCATION \\n \\nmba in human resource \\n \\nsikkim manipal university -  Bengaluru, Karnataka \\n \\nJune 2012 to June 2014 \\n \\nB.COM. in Marketing, Accountancy \\n \\nCollege Bangalore University -  Bengaluru, Karnataka \\n \\n2011 \\n \\nSKILLS \\n \\nPMP trained six sigma yellow belt \\n \\n\\n\\fLINKS \\n \\nhttps://www.linkedin.com/in/arjun-k-s-31388627/ \\n \\nADDITIONAL INFORMATION \\n \\nAreas of Expertise \\n \\nhttps://www.linkedin.com/in/arjun-k-s-31388627/ \\n \\n \\n\\u2611 Part of Recruitment team to hire process executives for Organization. \\n\\u2611 Possess excellent interpersonal, communication and organizational \\nskills with proven abilities \\nin team management, customer relationship management and planning. \\n\\u2611 Able to coordinate with different support teams like Training, MIS, \\nTechnology and Quality to \\nsuccessfully implement projects \\n\\u2611 Able to manage teams as per SOPs (Standard Operating Procedures) ISO \\n[\\u2026] and ensure \\ncompliance to SLA's and international standards. \\n\\u2611 Overseeing operations and ensuring achievement of desired objectives. \\n\\u2611 Driving day-to-day functions with key focus on bottom line \\nprofitability by ensuring optimal \\nResource utilization. \\n\\u2611 People Management & Team Building \\n\\u2611 Training, development, work allocation & goal setting and performance \\nappraisal of executives. \\n\\u2611 Putting systemic quality monitoring procedures in place to ensure \\nSLA's are met & exceeded. \\n\\u2611 Anticipate, organize & present information needed by management & \\nclient. Putting Security \\nmeasures in place to ensure information security & data integrity. \\n \\nAchievements \\nMultiple appreciations from Managers and Requesters for being customer \\ncentric and proactive \\n\\u2611 Throughout the carrier rating was \\\"Significantly above the peer group\\\" \\n\\u2611 Received number of appreciation mails from client and as well from \\nmanagement. \\n \\nProfessional Competence \\n\\u2611 Hardworking Team Player with good communication & interpersonal \\nskills. \\n\\u2611 Mentored new joiners, organized sessions and training for the process. \\n\\u2611 Willing to learn new concepts and take up larger responsibilities. \\n\\u2611 Have good understanding of SLA and how they relate to my performance \\nin order to deliver \\naccording to client expectations. \\n\\u2611 Have working knowledge of staffing practices at the shift level to \\nmanage staffing to meet \\nday to day needs. \\n\\u2611 Have broad understanding of complaint handling guidelines and \\nprocedures in use to resolve \\nand/or escalate relevant issues. \\n\\u2611 Have proactively identified opportunities to exceed goals and targets, \\nrecovered from setbacks \\n\\n\\fquickly, and identified newer ways to optimize resources needed to attain \\nobjectives. \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume1.pdf\",\n",
            "                \"text\": \"Afreen Jamadar \\nActive member of IIIT Committee in Third year \\n \\nSangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-\\nJamadar/8baf379b705e37c6 \\n \\nI wish to use my knowledge, skills and conceptual understanding to create \\nexcellent team \\nenvironments and work consistently achieving organization objectives \\nbelieves in taking initiative \\nand work to excellence in my work. \\n \\nWORK EXPERIENCE \\n \\nActive member of IIIT Committee in Third year \\n \\nCisco Networking -  Kanpur, Uttar Pradesh \\n \\norganized by Techkriti IIT Kanpur and Azure Skynet. \\nPERSONALLITY TRAITS: \\n\\u2022 Quick learning ability \\n\\u2022 hard working \\n \\nEDUCATION \\n \\nPG-DAC \\n \\nCDAC ACTS \\n \\n2017 \\n \\nBachelor of Engg in Information Technology \\n \\nShivaji University Kolhapur -  Kolhapur, Maharashtra \\n \\n2016 \\n \\nSKILLS \\n \\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 \\nyear), MICROSOFT \\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year) \\n \\nADDITIONAL INFORMATION \\n \\nTECHNICAL SKILLS: \\n \\n\\u2022 Programming Languages: C, C++, Java, .net, php. \\n\\u2022 Web Designing: HTML, XML \\n\\u2022 Operating Systems: Windows [\\u2026] Windows Server 2003, Linux. \\n\\u2022 Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql. \\n \\nhttps://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-\\ndownload&ikw=download-top&co=IN \\n\\n\\f\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"We_are_looking_for_a_Big_Data_Engineer##03-04-2020  18-10-16_SET\": {\n",
            "        \"job_description\": {\n",
            "            \"file_name\": \"We_are_looking_for_a_Big_Data_Engineer##03-04-2020  18-10-16.pdf\",\n",
            "            \"text\": \"We are looking for a Big Data Engineer. Looking for an enthusiastic person to help migrate Smart towards \\n\\nmore of a streaming architecture between the various data sources. \\n\\nWhat You\\u2019ll Need: \\n\\n\\u2022  Architect Level: 10+ years\\u2019 experience in Advanced Analytics and Big Data with the minimum \\n\\nimplementation of 2-3 projects at the enterprise level. \\n\\n\\u2022  Hands-on experience in architecting solutions at scale in big data. \\n\\n\\u2022  Solid functional understanding of the Open-Source Big Data Technologies including major Kafka \\n\\nprojects. \\n\\n\\u2022  Proficient with SQL, Druid, Spark. \\n\\n\\u2022  Database design and modeling - logical and physical. \\n\\n\\u2022  Performance tuning - table partitioning and indexing, process threading. \\n\\n\\u2022  Hands-on experience with \\\"commercializing\\\" Hadoop applications (e.g. administration, security, \\n\\nconfiguration management, monitoring, debugging, and performance tuning). \\n\\n\\u2022  Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the \\n\\nnew product capabilities, through to continuous delivery to production. \\n\\n\\u2022  Must have worked with Machine Learning and Data Sciences applications in downstream with very \\n\\nstrong experience in Python, SPARK Framework and deploying applications in the cloud. \\n\\n\\u2022  Detail-oriented with strong analytical and problem-solving skills. \\n\\n\\u2022  Effective communicator (both verbal & written). \\n\\n \\n\\n \\n\\nWelcome to a world of opportunity. Welcome to Manpower! \\n\\nAre you looking for new and exiciting career challenges? Manpower helps you take the right street \\n\\nFind the answer to your hiring challenges - see who's looking for your skills! Register now at \\n\\nDescrierea jobului \\n\\nDescrierea companiei \\n\\non the road to success! \\n\\nwww.manpower.ro! \\n\\n\\fManpowerGroup Romania is one of the leaders in innovative workforce solutions, creating and \\n\\ndelivering services that help clients meet their business and workforce objectives while enhacing \\n\\ntheir competitiveness. \\n\\nManpowerGroup is present in Romania since 2003, with branches in Bucharest, Timisoara, Brasov, \\n\\nCluj-Napoca, Ia\\u0219i, Craiova, Ploie\\u0219ti and Pite\\u0219ti. \\n\\nFor more information, you can access www.manpower.ro or visit our visit us on Facebook at \\n\\nManpowerGroup Romania. \\n\\n \\n\\n\\f\"\n",
            "        },\n",
            "        \"resumes\": [\n",
            "            {\n",
            "                \"file_name\": \"Resume18.pdf\",\n",
            "                \"text\": \"Dipesh Gulati \\nCo-coordinator of CODE HUNTER at CGC FEST \\n \\nDelhi, Delhi - Email me on Indeed: indeed.com/r/Dipesh-\\nGulati/17a483e9e19f9106 \\n \\nTo work in challenging environment that will provide opportunities for \\nlearning and growth with \\nthe best efforts for achieving organizational goals and at the same time \\nhaving self-development \\nthrough learning, exposure and seeking my own identity. \\n \\nWORK EXPERIENCE \\n \\nCo-coordinator of CODE HUNTER at CGC FEST \\n \\nORACLE SQL - \\n \\n2015 to 2015 \\n \\nEDUCATION \\n \\nMCA \\n \\nPunjab Technical University -  Chandigarh, Chandigarh \\n \\n2017 \\n \\nBCA \\n \\nBharati Vidyapeeth University -  Pune, Maharashtra \\n \\n2014 \\n \\nCBSE \\n \\nCrescent Public School -  New Delhi, Delhi \\n \\n2011 \\n \\nCBSE \\n \\nCrescent Public School -  New Delhi, Delhi \\n \\n2009 \\n \\nSKILLS \\n \\nACCESS (Less than 1 year), BUYING (Less than 1 year), BUYING/PROCUREMENT \\n(Less than 1 \\nyear), CSS (Less than 1 year), DATABASE (Less than 1 year) \\n \\nADDITIONAL INFORMATION \\n \\nI.T Skills \\n \\nhttps://www.indeed.com/r/Dipesh-Gulati/17a483e9e19f9106?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\n\\f\\u2022 Operating Systems: Windows-7, 10 \\n\\u2022 Language: Java \\n\\u2022 Database: SQL Server \\n\\u2022 Other Technologies: HTML, CSS, Microsoft Office (Word, Excel, \\nPowerPoint, Access) \\n \\nInternship \\n\\u2022 Organization: Global Infotech, Noida \\n\\u2022 Duration: 6 Months \\n \\nProjects Handled - SHOPPINGKART \\n1. Project Environment: Online Shopping \\n2. Language Used: CSS, JAVASCRIPTING, SQL SERVER and HTML. \\n \\nProject Description: \\nThe Project aim is to make the online shopping store by using languages \\nlike HTML, Java scripting \\netc. \\nIt is used by the internet users (customers) so that they can order the \\nproducts and make deals \\nat a click. \\nThe store has facilities for buying goods from the website. Further we \\ncan add delivery options in \\nthe website also. \\n \\nKey Responsibilities: \\n\\u2022 Involved in Analysis and developing the online store. \\n\\u2022 Interacting extensively with end users on requirement gathering, \\ndevelopment and \\ndocumentation. \\n\\u2022 Involved with key departments to analyze areas and discuss the primary \\nrequirements for the \\nproject. \\n\\u2022 Developed complex codes to simplify the online store interface. \\n\\u2022 Imported Data from relational database into Java as per detailed \\nspecifications. \\n \\nStrengths: \\n\\u2022 Disciplined, dedicated and hardworking with an ability to easily adapt \\nto changing work \\nenvironments and technologies. \\n\\u2022 Keen learner with ability to learn new knowledge with ease. \\n\\u2022 Good Inter-Personal and Communication Skills. \\n\\u2022 Confident. \\n\\u2022 Good presentation skills. \\n\\u2022 Good team working ability. \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume19.pdf\",\n",
            "                \"text\": \"Dushyant Bhatt \\nBI / Big Data/ Azure \\n \\nHyderabad-Deccan, Telangana, Telangana - Email me on Indeed: \\nindeed.com/r/Dushyant- \\nBhatt/140749dace5dc26f \\n \\n\\n\\u2022 10+ years of Experience in Designing, Development, Administration, \\nAnalysis, Management in \\nthe Business Intelligence Data warehousing, Client Server Technologies, \\nWeb-based Applications, \\ncloud solutions and Databases. \\n\\n\\u2022 Data warehouse: Data analysis, star/ snow flake schema data modeling \\nand design specific to \\ndata warehousing and business intelligence environment. \\n\\u2022 Database: Experience in database designing, scalability, back-up and \\nrecovery, writing and \\noptimizing SQL code and Stored Procedures, creating functions, views, \\ntriggers and indexes.  \\n\\u2022 Cloud platform: Worked on Microsoft Azure cloud services like Document \\nDB, SQL Azure, Stream \\nAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data \\nlake analytics(U-SQL). \\n\\n\\u2022 Big Data: Worked Azure data lake store/analytics for big data \\nprocessing and Azure data factory \\nto schedule U-SQL jobs. Designed and developed end to end big data \\nsolution for data insights.  \\n\\u2022 BI: \\no ETL: Designed and developed ETL solution in SSIS. Experience in \\nLogging, Error handling, \\nconfiguration, deployment, troubleshooting and performance tuning of SSIS \\nPackages. \\no Reporting: Experience in all the Latest Reporting Tools like Tableau \\nData visualization, Power \\nBI and SSRS 2012. Act as a Point of Contact in Data Interoperability, \\nAnalytics and BI and \\nProduction Support issue resolution. Experience in Developing Performance \\nDashboards, Score \\ncards, Metrics, what if analysis, Prompts, Drills. Reports/Dashboards for \\nall the functional areas \\nincluding Finance, Pricing, Purchasing and Sales/Marketing. \\n \\nWilling to relocate: Anywhere \\n \\nWORK EXPERIENCE \\n \\nSoftware Engineer \\n \\nMicrosoft -  hyderbad, Telangana - \\n \\nDecember 2015 to Present \\n \\n1. Microsoft Rewards Live dashboards: \\nDescription: - Microsoft rewards is loyalty program that rewards Users \\nfor browsing and shopping \\nonline. Microsoft Rewards members can earn points when searching with \\nBing, browsing with \\nMicrosoft Edge and making purchases at the Xbox Store, the Windows Store \\nand the Microsoft \\n\\n\\fStore. Plus, user can pick up bonus points for taking daily quizzes and \\ntours on the Microsoft \\nrewards website. Rewards live dashboards gives a live picture of usage \\nworld-wide and by \\nmarkets like US, Canada, Australia, new user registration count, \\ntop/bottom performing rewards \\noffers, orders stats and weekly trends of user activities, orders and new \\nuser registrations. the \\nPBI tiles gets refreshed in different frequencies starting from 5 seconds \\nto 30 minutes. \\nTechnology/Tools used \\n\\u2022 Event hub, stream analytics and Power BI. \\nResponsibilities \\n\\u2022 Created stream analytics jobs to process event hub data \\n \\nhttps://www.indeed.com/r/Dushyant-Bhatt/140749dace5dc26f?isid=rex-\\ndownload&ikw=download-top&co=IN \\nhttps://www.indeed.com/r/Dushyant-Bhatt/140749dace5dc26f?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\u2022 Created Power BI live dashboard to show live usage traffic, weekly \\ntrends, cards, charts to show \\ntop/bottom 10 offers and usage metrics. \\n \\n2. Microsoft Rewards Data Insights: \\nDescription: - Microsoft rewards is loyalty program that rewards Users \\nfor browsing and shopping \\nonline. Microsoft Rewards members can earn points when searching with \\nBing, browsing with \\nMicrosoft Edge and making purchases at the Xbox Store, the Windows Store \\nand the Microsoft \\nStore. Plus, user can pick up bonus points for taking daily quizzes and \\ntours on the Microsoft \\nrewards website. Rewards data insights is data analytics and reporting \\nplatform, processes 20 \\nmillion users daily activities and redemption across different markets \\nlike US, Canada, Australia. \\nTechnology/Tools used \\n\\u2022 Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power \\nBI. \\nResponsibilities \\n\\u2022 Created big data scripts in cosmos \\n\\u2022 C# data extractors, processors and reducers for data transformation \\n\\u2022 Power BI dashboards \\n \\n3. End to end tracking Tool: \\nDescription: - This is real-time Tracking tool to track different \\nbusiness transactions like order, \\norder response, functional acknowledgement, invoice flowing inside ICOE. \\nIt gives flexibility to \\ncustomers to track their transactions and appropriate error information \\nin-case of any failure. \\nBased on resource based access control the tool gives flexibility to end \\nuser to perform different \\nactions like view transactions, search based on different filter criteria \\nand view and download \\nactual message payload. End to end tracking tool stitches all the \\nbusiness transaction like order \\n\\n\\fto cash flow and connects different hops inside ICOE like gateway, \\nrouting server, Processing \\nserver. It also connects different systems like ICOE, partner end point \\nand SAP. \\nTechnology/Tools used \\n\\u2022 Azure Document db, Azure web job and Web APP, RBAC, Angular JS. \\nResponsibilities \\n\\u2022 Document dB stored procedures. \\n\\u2022 Web job to process event hub data and populate Document db \\n\\u2022 Web App API. \\n\\u2022 Stream analytics job to transform data \\n\\u2022 Power BI reports \\n \\n4. Biztrack Tracking Tool: \\nDescription: - This is real-time Tracking tool to track different \\nbusiness transactions like order, \\norder response, functional acknowledgement, invoice flowing inside ICOE. \\nIt gives flexibility to \\ncustomers to track their transactions and appropriate error information \\nin-case of any failure. \\nBased on resource based access control the tool gives flexibility to end \\nuser to perform different \\nactions like view transactions, search based on different filter criteria \\nand view and download \\nactual message payload. \\nTechnology/Tools used \\n\\u2022 SQL server 2014, SSIS, .net API, Angular JS. \\nResponsibilities \\n\\n\\u2022 ETL solution to transform business transactions data stored in Biztalk \\ntables. \\n\\u2022 SQL azure tables, stored procedures, User defined functions. \\n\\u2022 Performance tuning. \\n\\u2022 Web API enhancements. \\n \\n \\n \\nEDUCATION \\n \\nSaurashtra University -  Morbi, Gujarat \\n \\n2007 \\n \\nSKILLS \\n \\nproblem solving (Less than 1 year), project lifecycle (Less than 1 year), \\nproject manager (Less \\nthan 1 year), technical assistance. (Less than 1 year) \\n \\nADDITIONAL INFORMATION \\n \\nProfessional Skills \\n\\n\\u2022 Excellent analytical, problem solving, communication, knowledge \\ntransfer and interpersonal \\nskills with ability to interact with individuals at all the levels \\n\\u2022 Quick learner and maintains cordial relationship with project manager \\nand team members and \\ngood performer both in team and independent job environments \\n\\u2022 Positive attitude towards superiors & peers \\n\\u2022 Supervised junior developers throughout project lifecycle and provided \\ntechnical assistance. \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume16.pdf\",\n",
            "                \"text\": \"Dhanushkodi Raj \\nTechnology Analyst - Infosys Limited \\n \\nChennai, Tamil Nadu - Email me on Indeed: indeed.com/r/Dhanushkodi-\\nRaj/cf31bbac6c5a5d29 \\n \\nA highly competent and results oriented Senior Automation Test Analyst \\nwith 9+ years of \\nexperience in Software Testing, Selenium Automation, Development, JAVA \\nweb projects, Team \\nleading & training, Client facing. Proven ability in Selenium WebDriver \\nAutomated testing & \\nFrameworks, Page Objects & Hybrid Frameworks, TDD, Gherkin language \\n(BDD), Web testing, \\nFunctional testing, Performance & Load testing. Excellent experience in \\neach phase of Test \\nLife Cycle, Test strategies & Test plans, UAT. Solid experience of \\nSoftware Development Life \\nCycles, Selenium WebDriver, Cucumber, JAVA, JUnit, Web Applications, \\nJIRA, Maven, SQL, AGILE \\nMethodologies, Scrum, LoadRunner, E-commerce/Financial domains, \\nAutomation & Testing tools. \\n \\n\\u2022 Excellent experience in automation of Web-based, Highly transactional \\nand large Client/Server/ \\nMulti-tier applications \\n\\u2022 Proven ability in writing Selenium Webdriver using JAVA, TestNG, JUnit, \\nand Leading JAVA projects \\nautomation \\n\\u2022 Expertise on Manual testing of the applications \\n\\u2022 Solid experience on Selenium using Cucumber automation \\n\\u2022 Solid experience of AGILE Methodologies, AGILE Manifesto and Scrum \\nprocesses \\n\\n\\u2022 Solid experience of Defect/Test Management tools like JIRA, \\nQualityCenter. \\n\\u2022 Proven skills in SQL, RDBMS and UNIX variant like LINUX \\n\\u2022 Expertise in writing SQL Queries, SQL Scripts and performing Database \\ntesting \\n\\u2022 Proven skills in Leading and training teams, Test Management and \\nMeeting the deadlines \\n\\n\\u2022 Strong communication & interaction with Clients, Developers, Business \\nAnalysts, Management \\n\\u2022 Strong experience of E-commerce, Financial, Banking, Insurance, Trading \\nand Telecom domains \\n\\n\\u2022 Proven ability in Windows/UNIX Commands, Scripts and testing on \\nWindows/UNIX platforms \\n\\n\\u2022 Proven ability in Working individually or as a part of team, Problem \\nsolving, Process \\nimprovement activities, Analytical skills, Reporting and Assisting to \\nManager \\n \\nWORK EXPERIENCE \\n \\nTechnology Analyst \\n \\nInfosys Limited - \\n \\nJune 2015 to Present \\n \\n\\n\\fProject: Video Banking testing and Web Application testing for Barclays \\nBank \\nBarclays Bank is one of the leading banks in the UK. Video Banking \\nfeature Barclays Video Banking \\nlets you have a secure, confidential conversation with us. It's a \\nconvenient alternative to visiting \\na branch. Talk about account services; Ask mortgage-related questions; \\nHave appointments with \\nyour Relationship Manager. \\nAchievements: \\n\\u2022 Won Barclays Best Agile award for the year 2016 \\n\\u2022 Saved 30% time and cost of testing by automation using Selenium \\nWebDriver, JAVA, Cucumber \\n\\n\\u2022 Increased productivity by 20% by effectively training, leading and \\nmanaging the AGILE \\nautomation team \\n \\nhttps://www.indeed.com/r/Dhanushkodi-Raj/cf31bbac6c5a5d29?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\n\\u2022 Achieved customer satisfaction and high quality by effective Automation \\ntesting, AGILE \\nProcesses \\nResponsibilities: \\n\\u2022 Implemented automation using Selenium WebDriver, JAVA, Cucumber, Maven. \\n\\u2022 Extensively automated regression and functional test suites by \\ndeveloping over 237 test cases, \\n6 test suites using Selenium WebDriver, JAVA, JUnit. \\n\\u2022 Implemented Page Objects framework, Hybrid framework and 21 Page \\nclasses from scratch to \\nrepresent web pages. \\n\\n\\u2022 Developed Data Driven frameworks to retrieve test actions, test data \\nfrom Excel files and SQL \\nDatabases. \\n\\n\\u2022 Configured Maven for JAVA automation projects and developed Maven \\nproject object model \\n(POM) \\n\\n\\u2022 Developed BDD tests using Cucumber by writing behaviours and step \\ndefinitions. Developed \\nrequired Selenium support code in JAVA for Cucumber. \\n\\n\\u2022 Wrote SQL queries extensively, queried database and generated test \\nreports. Performed \\nPurchase Orders Database testing by developing 14 SQL scripts. \\n\\n\\u2022 Performed Defect Tracking & Management in JIRA. Generated automated \\ndaily reports using \\nJIRA API. \\n\\n\\u2022 Worked in a highly dynamic AGILE environment and participated in scrum \\nand sprint meetings \\n\\u2022 Assisted Manager by providing automation strategies, Selenium/Cucumber \\nAutomation and \\nJIRA reports. \\n\\n\\u2022 Identified weaknesses in QA Processes, Web testing, Selenium \\nAutomation. Suggested & \\nimplemented improvements. \\n \\nTools/Environments: Selenium WebDriver, Maven, JAVA, TestNG, JIRA, HP \\nQualityCenter, \\nConfluence page, SQL, Oracle \\n \\n\\n\\fTechnology Analyst \\n \\nInfosys Limited -  Chennai, Tamil Nadu - \\n \\nJanuary 2013 to June 2015 \\n \\nProject: Automation on Web Application for PGE project \\nWith the implementation of the new platform we did not see the uplift in \\ncall containment that \\nwas expected. Over the course of the year following deployment, several \\nchanges were made \\nin an attempt to improve the containment rate. Based on the analysis done \\nduring this time we \\ndetermined that moving away from the natural language speech platform and \\nback to a directed \\ndialog menu structure would get us back on \\nAchievements: \\n\\n\\u2022 Saved 20% of testing budget by automating Regression/Functional tests \\nusing Selenium \\nWebDriver & Java \\nResponsibilities: \\n\\n\\u2022 Automated System testing for trading transactions, exchange operations \\nand payment methods \\nusing Selenium WebDriver, Java. \\n\\n\\u2022 Developed, executed and maintained over 182 Selenium automation scripts \\nfor trading web \\napplication. \\n\\u2022 Developed Hybrid automation framework Java by using Page Objects \\nframework, Data Driven \\nframeworks. Automated running smoke tests and build report generator for \\ndaily builds. \\n\\n\\u2022 Developed 7 Java class libraries, 16 JUnit test scripts to test XML \\nbuild data files. \\n \\n \\n \\n\\n\\u2022 Performed Load and Stress testing by developing LoadRunner scripts to \\nfind out effective Web/ \\nApp server configurations for best performance levels. \\n\\n\\u2022 Extensively performed Database testing using Selenium. Generated \\nProduction DB reports \\nusing SQL queries. \\n\\n\\u2022 Provided different build reports, Selenium automation reports, \\nPerformance testing reports to \\nSenior Management. \\n\\n\\u2022 Worked effectively with Developers, AGILE Team, Project Management to \\nachieve Selenium \\nautomation, high quality, release deadlines and QA processes \\nimprovements. \\n \\nTools/Environments: Selenium WebDriver, JAVA, Junit, Maven, JIRA, HP \\nQualityCenter, MySQL \\n \\nSoftware QA Engineer \\n \\nInfosys Limited -  Chennai, Tamil Nadu - \\n \\nJune 2012 to December 2012 \\n \\n\\n\\fProject: Corporate e-banking system is a comprehensive corporate and \\nsmall business banking \\nsolution providing a single unified view of the corporate banking \\nrelationships across asset and \\nliability products, limits, trade finance and cash management. Corporate \\ncustomers of the bank \\ngain the flexibility to view details of each account, capability to make \\nInter-bank and Intra-bank \\npayments, access to trade finance information and transactions, and the \\nability to perform cash \\nmanagement activities. This also supports comprehensive authorization \\nworkflow, Transaction \\nLimits, Electronic Invoice Presentment and Payment (EIPP) and Corporate \\nAdministration. \\n \\nIt is also highly secure and provides support for different \\nauthentication mechanisms \\nResponsibilities: \\n\\u2022 Involved in Automation Test Plan Preparation. \\n\\u2022 Test Design Based on the Use Case Specifications. \\n\\u2022 Generating Test Scenarios, Test case documentation and test data \\ncollection. \\n\\u2022 Involved in Tests Execution and Reporting Defects Using Sun-Tracker. \\n\\u2022 Responsible for GUI, Functional and System Testing. \\n\\u2022 Involved in Compatibility Testing. \\n\\u2022 Performed Database Testing using SQL in order to check the Data \\nValidation and Data Integrity. \\n\\u2022 Involved Peer reviews (Test Design) \\n\\u2022 Clear look on updating concepts and finding defects, sending it with \\nsuggestions. \\n \\nTools/Environments: Java, Jboss, Oracle and Win2008, SunTracker \\n \\nSoftware QA Engineer \\n \\nInfosys Limited -  Chennai, Tamil Nadu - \\n \\nOctober 2010 to May 2012 \\n \\nProject: Alert messaging is the process of reliably and securely \\ncirculating messages associated \\nwith an event to interested users. Due to the growth of Information \\nTechnology (IT) and \\ntelecommunications sectors in the recent past, the dissemination of alert \\nmessages has become \\nprompt and easier. \\nSince the existing business environment faces the challenge of reaching \\nincreasingly segmented \\nmarkets, the design, delivery, and analysis of business communications \\nneeds to be tightly \\n \\n \\n \\nintegrated across multiple alert notification channels. Telecommunication \\nchannels through \\nwhich subscribers can receive alert messages may include email, fax, text \\nmessaging, instant \\nmessaging, broadband network and the like. Due to myriad software \\napplications implemented by \\n\\n\\fbusinesses and the vast demographics of customers, it may become \\nnecessary to segment alert \\nmessages so that the messages can be sent through channels based on \\nbusiness needs as well \\nas customer preferences. Further, in order to maintain strong customer \\nrelationships, businesses \\nneed to time the messages precisely. This brief illustrates how Alert \\nNotification System (IANS) \\ncan help organization leverage existing infrastructure to better service \\ntheir customers. \\n \\nAchievements: \\n\\u2022 Won a performance certificates and given a bonus for outstanding \\nperformance \\n\\n\\u2022 Received Onsite opportunity for supporting the production and DR \\nenvironment at Saudi Arabia \\n \\nResponsibilities: \\n\\n\\u2022 Manual testing of the application, Single Point of contact for two of \\nthe clients. \\n\\u2022 Understood and prepared functional testcases for SMS Inbound with the \\nprovided requirements. \\n\\u2022 Completed the testbed setup and Functional Testing of the Inbound SMS. \\n\\u2022 Written two Jsps and two servlets for storing and retrieving the data \\nfrom DB. Implemented the \\nstoring and retrieving of Data from DB in Struts concept \\n\\n\\u2022 Successfully completed the testbed setup for all the Virtual machines \\nallocated for the project. \\nHelped the team for functional and performance testing. \\n\\u2022 Involved in Performance testing with 3 Virtual Machines setup using \\nJmeter (Distributed testing) \\n\\u2022 Developed the ANS Management console using Grails and Groovy. \\n\\u2022 Worked effectively in the development of 6 modules using Grails and \\nGroovy. \\n \\nTools/Environments: Java, Servlets, Jsps, Grails and Groovy, Manual \\nTesting using Soap UI, Jmeter, \\nPerformance testing. \\n \\nSoftware Test Analyst \\n \\nInfosys Limited -  Chennai, Tamil Nadu - \\n \\nJanuary 2009 to October 2010 \\n \\nProject: PoPs (Purchase order processing System) facilitates the function \\nof buying materials / \\ncomponents / products / raw materials at economical cost in a timely \\nmanner. This will have \\nbroad functionalities of request handling, obtaining quotations including \\ncomparison, placing \\npurchase orders and follow-up of purchase orders for timely deliveries, \\nsupplier information, \\nvendor wise supply analysis, and purchase payments. This covers all type \\nof Enquiry, Quotations, \\nComparisons, and Approval of Purchase order. Sending PO to Supplier, \\nmaintaining Bills. \\nResponsibilities: \\n\\u2022 Understanding the Business Requirement Specifications. \\n\\n\\f\\u2022 Designing Test Cases, Execution of the test cases and reporting the \\nbugs. \\n\\n\\u2022 Conducted Functional testing with Valid and Invalid inputs for positive \\nand negative testing. \\n\\u2022 Communicates with Functional Analyst on any issue clarifications. \\n\\u2022 Preparing Defect report as per severity and priority Active \\nparticipation in Bug triage meetings. \\n\\u2022 Modifying the Tests and Conducting Regression Testing. \\n\\u2022 Participation in Test Closure activities. \\n \\nTools/Environments: VB.NET, SQL Server and Windows2008, Bugzilla \\n \\n \\n \\nEDUCATION \\n \\nBachelor's in Computer Science \\n \\nSri Venkateshwara College of Engineering -  Chennai, Tamil Nadu \\n \\n2004 to 2008 \\n \\nSKILLS \\n \\nTESTING (9 years), JAVA (3 years), JUNIT (2 years), ORACLE (2 years), API \\n(1 year) \\n \\nLINKS \\n \\nhttps://www.linkedin.com/in/dhanushkodi-raj-a190a0155 \\n \\nADDITIONAL INFORMATION \\n \\nTECHNICAL SKILLS \\nAutomation Testing Tools: Selenium WebDriver, Cucumber, Maven, Jenkins, \\nCyara \\nLanguages/Frameworks: JAVA, Data Driven, Hybrid, Page Object model, SOAP \\nUI, JUnit, TestNG \\n(TDD), Gherkin language (BDD), Cucumber, Log 4j \\nDefect Tracking Tools: JIRA, HP ALM \\nAPI Webservices Automation Skills: SOAP, REST, HttpClient, SOAP UI, XML, \\nJmeter, Postman \\nTest Management Tools: HP ALM, Confluence page \\nRDBMS: MySQL, Oracle, Mongo DB \\nSource Control Management: SVN, GIT \\nOperating Systems: Windows \\nDomain Knowledge: E-commerce, Financial, Banking, Telecom, Billing \\n \\nhttps://www.linkedin.com/in/dhanushkodi-raj-a190a0155 \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume17.pdf\",\n",
            "                \"text\": \"Dinesh Reddy \\nDeployed chef for configuration management infrastructure - Cisco \\nBangalore \\n \\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Dinesh-\\nReddy/139711455c45e1ad \\n \\nTo give my best in my professional pursuit for overall benefit and growth \\nof the company that I \\nserve by facing the challenges. I will show my caliber and gain some \\nexperience. \\n \\nProfessional Credentials and Abilities \\n\\n\\u2022 Having 3.5+ years of experience as a Build and Release Engineer in \\nautomating, building, \\nreleasing of code from one environment to other environment and deploying \\nto servers. \\n \\n\\n\\u2022 Extensively worked on Jenkins for continuous integration and for End to \\nEnd automation for all \\nbuild and deployments. \\n \\n\\u2022 Having In-depth understanding of the principles and best practices of \\nSoftware Configuration \\n \\n\\n\\u2022 Extensive experience using MAVEN and ANT as build tools for the \\nbuilding of deployable artifacts \\n(jar, war & tar) from source code. \\n \\n\\n\\u2022 Build, Configure, Manage and Coordinate all Build and Release \\nManagement activities. \\n \\n\\u2022 Worked on Automated build & Release process. \\n \\n\\u2022 Automated highly build, test, and reporting mechanisms. \\n \\n\\n\\u2022 Worked on server machines with platforms like windows64, windows32 and \\nlinux64 to test and \\nresolve the new release for group build issues, test failures, build \\nbreaks, Integration issues, and \\nanalyzed test logs in Agile Environment. \\n \\n\\n\\u2022 Troubleshooting and problem solving of Linux/UNIX servers, debugging OS \\nfailure. \\n \\n\\u2022 Experience in working on source control tools like SVN and GIT. \\n \\n\\n\\u2022 Expert in deploying the code through web application servers like \\nApache, Tomcat. \\n \\n\\n\\u2022 Extensive experience of working with the release and deployment of \\nlarge scale Java/J2EEWeb \\napplications. \\n \\n\\u2022 Experience in Deploying to and administering in the use of Tomcat, \\nNexus, Apache web server, \\nSVN. \\n\\n\\u2022 Excellent communicative, interpersonal, intuitive, analysis and \\nleadership skills with ability to \\nwork efficiently in both independent and team work environments. \\n\\n\\f \\nWilling to relocate to: Bengaluru, Karnataka - anywhere \\n \\nhttps://www.indeed.com/r/Dinesh-Reddy/139711455c45e1ad?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\nWORK EXPERIENCE \\n \\nDeployed chef for configuration management infrastructure \\n \\nCisco Bangalore -  Bengaluru, Karnataka - \\n \\nSeptember 2016 to Present \\n \\nRole: Build & release. \\nEnvironment: Java 1.7, Subversion 1.8, Ant 1.9.4, GIT 2.1, Puppet, Jira \\n6.3, Nexus, Shell scripting, \\nweb logic 11g and Tomcat Servers 8. \\nResponsibilities: \\n\\u2022 Coordinate with the Development, Database Administration, QA, and IT \\nOperations teams to \\nensure there are no resource conflicts. \\n\\n\\u2022 Installed and configured Jenkins for Automating build, deployments and \\ntest execution and \\nproviding a complete automation solution using jenkins. \\n\\u2022 Deployed chef for configuration management infrastructure, Implemented \\nauto environment \\ncreation using chef. \\n\\n\\u2022 Designing and implementing fully automated server build, management, \\nmonitoring and \\ndeployment solutions spanning multiple platforms, tools and technologies \\nincluding Jenkins \\nNodes/Agents, SSH, deployment and testing. \\n\\n\\u2022 Created repositories according the structure required with branches, \\ntags and trunks. \\n\\n\\u2022 Responsible for maintaining Multiple DevOps tools & configuring across \\nall projects Subversion \\n(SVN), GIT, Maven, Jenkins, ANT, Artifactory, Chef. \\n\\u2022 Configured application servers (WebLogic) to deploy the code. \\n\\u2022 Experience in deploying applications on Tomcat and Web logic servers. \\n\\u2022 Managed and document all post deployment issues utilizing the Post \\nDeployments Issue Log. \\n \\nDeclaration \\nI hereby declare that the statements made herein are true, complete and \\ncorrect to the best of \\nmy knowledge and belief. I have not suppressed any material fact or \\nfactual information herein. \\n \\nPlace: Bangalore \\nDate: \\n \\nsoftware engineer \\n \\ncisco - \\n \\nJune 2014 to Present \\n \\nbuild & release engineer \\n\\n\\f \\nDevops Engineer \\n \\nCisco Bangalore -  Bengaluru, Karnataka - \\n \\n2014 to Present \\n \\nTechnical Skills \\nDevops pipelines: Cloud bees Enterprise Jenkins, GIT, Docker, AWS \\nSCM Tools: SVN, GIT. \\n \\n \\n \\nContinuous Build: ANT, Maven \\nArtifact Repository: Sontype Nexus \\nContainers: Docker, AWS Basics, Chef \\nStatic code Analysis: Sonarqube Check style \\nCI Tools: Cloud bees Enterprise Jenkins. \\nEditors and IDEs: Eclipse (IMS Distribution), Notepad++ \\nOperating Systems: MS Windows XP/2000/NT/98, UNIX, Linux \\nDatabase: MY SQL. \\nServers: Apache, Tomcat \\nTools: MS Office \\n \\nsoftware engineer \\n \\ncisco \\n \\nBuild & Release through complete automation using jenkins and config by \\nchef in complete cloud \\nenviornment like AWS GCP \\n \\nEDUCATION \\n \\nBSC \\n \\nSri Venkateswara university tirupati -  Tirupati, Andhra Pradesh \\n \\n2012 \\n \\nSKILLS \\n \\nAWS Devops \\n \\nADDITIONAL INFORMATION \\n \\nGit maven jenkins vagrant docker nagios sonarcube chef ansible \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume14.pdf\",\n",
            "                \"text\": \"Bhawana Daf \\nPune, Maharashtra - Email me on Indeed: indeed.com/r/Bhawana-\\nDaf/d9ddb6a54519d583 \\n \\nSeeking a career in preschool where I can utilize my teaching background \\nstrongly ,in nurturing \\nyoung minds ,demonstrating quality teaching skills. \\n \\nWilling to relocate to: Viman Nagar, Maharashtra - Vadgaonsheri - \\nKharadi, Maharashtra \\n \\nWORK EXPERIENCE \\n \\nPreschool Teacher \\n \\nPune, Maharashtra - \\n \\n2015 to 2018 \\n \\nClass teacher \\n \\nData Entry and Discrepancy Management \\n \\nOracle Clinical \\n \\n4.6) \\n\\u2022 Knowledge of clinical trial data like Demographic Data, Adverse Events \\n(AE), Serious Adverse \\nEvents (SAE), Laboratory Data (Lab Data) etc. \\n \\nEDUCATION \\n \\nB. Sc. in Biology \\n \\nGovernment Science College -  Pandhurna, Madhya Pradesh \\n \\nH.S.C \\n \\nR.D.H.S. School -  Chhindwara, Madhya Pradesh \\n \\nPost Graduate Diploma in Clinical Data Management \\n \\nR.D.H.S. School -  Chhindwara, Madhya Pradesh \\n \\nSKILLS \\n \\nTeaching (3 years) \\n \\nADDITIONAL INFORMATION \\n \\nSKILLS: \\n\\u2022 Perform all functions related to Data Entry like First Pass and Second \\nPass Data Entry, review \\nof Data Entry, and database update. Well versed in reading hand written \\npatient documents. \\n \\nhttps://www.indeed.com/r/Bhawana-Daf/d9ddb6a54519d583?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\n\\f\\u2022 Review, analyze, and validate clinical trial data to ensure \\nconsistency, integrity and accuracy \\nbased on project specific guidelines. \\n \\n\\n\\u2022 Maintain clinical trial data accuracy through review of case report \\nforms for completeness and \\nconsistency. \\n \\n\\u2022 Query data inconsistencies and revise case report forms in compliance \\nwith standard operating \\nprocedures, client guidelines and regulatory agency guidelines. \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume13.pdf\",\n",
            "                \"text\": \"Ayushi Srivastava \\nSenior Analyst - Cisco \\n \\nNew Delhi, Delhi - Email me on Indeed: indeed.com/r/Ayushi-\\nSrivastava/2bf1c4b058984738 \\n \\nIT Professional with an experience of 1year and 10 months (22 months) . I \\nam looking forward to \\nwork in a competitive environment and enhance my skills of networking and \\nwork towards the \\ngrowth of the organization. \\n \\nWilling to relocate: Anywhere \\n \\nWORK EXPERIENCE \\n \\nSenior Analyst \\n \\nCisco - \\n \\n2016 to Present \\n \\nWorking in the network Voip team as a tier2 engineer. \\nCCNA Certified. \\nKnowledge of PBx, telephone issue troubleshooting. \\nPerform troubleshooting on all endpoints and have direct interaction with \\ncustomers \\nManaging all the Cisco, Tandberg and Polycom telepresence endpoints. \\nWorking on TMS (Telepresence Management Suit), VCS (Video communication \\nServer) and CUCM \\n(Cisco unified call manager) \\nHaving knowledge about network protocols like SIP, SCCP and H.323. \\nExperience of configuration and troubleshooting Cisco routers and \\nswitches. \\nExperience of using RTMT tool for monitoring of CUCM and taking logs. \\n \\nEDUCATION \\n \\nB.Tech \\n \\nHMR Institute of Technology and Management in Electronics and \\nCommunication \\n \\n2012 to 2016 \\n \\nSenior Secondary \\n \\n2012 \\n \\nSumermal Jain Public School \\n \\n2010 \\n \\nSKILLS \\n \\nCSS (Less than 1 year), DHCP (Less than 1 year), HSRP. (Less than 1 \\nyear), routing protocols. \\n(Less than 1 year), Voip (2 years) \\n \\n\\n\\fhttps://www.indeed.com/r/Ayushi-Srivastava/2bf1c4b058984738?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\nADDITIONAL INFORMATION \\n \\nTECHNICAL KNOWLEDGE \\n\\n\\u2022 Have knowledge of configuration of routers, switches, routing \\nprotocols. \\n\\n\\u2022 Knowledge of cucm including Call Search Space (CSS), Partitions and \\ndevice registration. \\n\\u2022 Knowledge of VLAN, port-securities, STP, ACL and protocols including \\nHSRP. \\n\\u2022 Knowledge of Subnetting, VLAN Tagging, DHCP \\n \\nAREA OF INTEREST \\n\\u2022 Work in the networks team and explore more in VoIP domain. \\n\\u2022 Gaining more knowledge about CUCM, routers and switching. \\n\\u2022 Gain more experience on managing the gateways. \\n \\nKEY STRENGTHS \\n\\u2022 Adaptive towards location changes. \\n\\u2022 Determination to work hard and learn new technologies. \\n\\u2022 Open to work in any shift. \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume15.pdf\",\n",
            "                \"text\": \"Darshan G. \\nFinancial Analyst - Oracle \\n \\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Darshan-\\nG/025a61a82c6a8c5a \\n \\nHard worker, Patience and Good commitment. \\n \\nI here by declare that the above-furnished details are true up to my \\nknowledge. \\n \\nPlace: Bangalore (Darshan M G) \\nDate: Signature \\n \\nWORK EXPERIENCE \\n \\nFinancial Analyst \\n \\nOracle - \\n \\nJune 2015 to Present \\n \\nRoles and responsibilities: \\n \\n\\u2022 Auditing. (As per T & E claims) \\n\\u2022 Catalogues (Export & import activity) \\n\\u2022 Payment validation. \\n\\u2022 Fall back audits. \\n\\u2022 Manual expenses (Inactive employees) \\n\\u2022 Handing queries. (E-mails) \\n\\u2022 Invoice processing \\n\\u2022 Handing payment queries. \\n\\u2022 Fringe benefit tax. \\n \\nCarrier Achievements; \\n\\u2022 Received Numerous Monthly and Quarterly awards for completing assigned \\ntask on time. \\n\\n\\u2022 Received numerous appreciation emails from Vendors for making On Time \\nPayment. \\n\\u2022 Received appreciations emails from Supervisor for knowing End-to-End \\nprocess and first point \\nof contact person for any escalation. \\n\\u2022 Submitted Innovative ideas to improve the process efficiency and \\nnominated for Internal Award. \\n \\nProcess associate \\n \\nAccenture - \\n \\nFebruary 2014 to May 2015 \\n \\nRoles & Responsibility; \\n\\u2022 Invoice backlog. \\n\\u2022 Overall hold summary. \\n\\u2022 Payment rejections. \\n\\u2022 Requiting backlog report. \\n \\nhttps://www.indeed.com/r/Darshan-G/025a61a82c6a8c5a?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n\\n\\f \\n\\u2022 PO stuck. \\n\\u2022 Expense hold. \\n\\u2022 Bank details invalid. \\n\\u2022 Schedule payment hold. \\n\\u2022 Work flow. \\n\\u2022 Daily report status. \\n\\u2022 Dash board update \\n \\nEDUCATION \\n \\nMBA in Finance \\n \\nAdhichunchanagiri Institute Of Technology -  Chikmagalur, Karnataka \\n \\n2013 \\n \\nB B M in Education \\n \\nI D S G GOVT. College -  Chikmagalur, Karnataka \\n \\n2010 \\n \\nUniversity / Board \\n \\nSKILLS \\n \\nExcel (Less than 1 year), MS Excel (Less than 1 year), Tally (Less than 1 \\nyear) \\n \\nADDITIONAL INFORMATION \\n \\nTechnical Skills: Oracle application (Rx11) & Cloud application. \\n \\nComputer skills: MS Excel, Tally \\n \\nProject: \\n \\nProject Title: Education loan scheme in credited system. \\nCompany name: Corporation bank, Bangalore \\nTeam size: 1 \\n\\n\\f\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"Job_Description_3##03-04-2020  18-11-36_SET\": {\n",
            "        \"job_description\": {\n",
            "            \"file_name\": \"Job_Description_3##03-04-2020  18-11-36.pdf\",\n",
            "            \"text\": \" \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nDescrierea jobului \\n\\nResponsibilities \\n\\nand orchestration. \\n\\nRequirements \\n\\nCandidatul ideal \\n\\neMAG is looking for a highly motivated colleague to join our AntiSpiders team as Data Scientist. \\n\\nOur team analyses web traffic, identifies non-human agents (crawlers, search engines) and \\n\\ndevelops pipelines for filtering data used by the Business Intelligence department. \\n\\nWe have built our applications with Spark, Kafka, BigQuery, and Airflow. They are mostly written in \\n\\nPython and deployed using Docker on a hybrid infrastructure (own machines and Google Cloud) \\n\\nYou will analyze large-scale datasets and build machine learning models to enhance spider \\n\\ndetection. These models will be integrated in our production pipelines. \\n\\nDiscover, interpret and document unique insights in large-scale distributed datasets through \\n\\nexploratory analysis and the application of advanced analytical methodologies; \\n\\nCreate statistical and machine learning models in multiple technologies using best-in-class data \\n\\nscience approaches; \\n\\nProcessing, cleansing, and verifying the integrity of data used for analysis; \\n\\nDevelop dashboards and reporting workflows for internal clients; \\n\\nWrite production-level code for training and inference: data validation, logging, exception handling, \\n\\nA degree in a related discipline (Mathematics, Statistics, Data Science, Computer Science); \\n\\n1 year of related work and/or research experience in quantitative roles; \\n\\nKnowledge of at least one open-source scientific or statistical programming language such as R or \\n\\nPython; \\n\\nFluency in SQL and good knowledge of relational databases; \\n\\n\\fExperience with data visualization tools. \\n\\nThe successfull candidate will: \\n\\nBe passionate about asking and answering questions in large, distributed datasets; \\n\\nBe a clear and confident communicator with the ability to translate technical concepts into a concise \\n\\nbusiness focused messages in both technical and management presentations; \\n\\nHave knowledge in the following data science domains: Anomaly Detection, Time Series Analysis, \\n\\nUnsupervised Learning; \\n\\nHave the ability to collaborate with an interdisciplinary team to solve problems; \\n\\nHave a good understanding of the design and architecture of big data applications; \\n\\nPossess a solid understanding of the value of data and how technology enables companies to \\n\\ncompete better in the market place. \\n\\n \\n\\n \\n\\n \\n\\n\\f\"\n",
            "        },\n",
            "        \"resumes\": [\n",
            "            {\n",
            "                \"file_name\": \"Resume7.pdf\",\n",
            "                \"text\": \"Arun Elumalai \\nQA Tester \\n \\nChennai, Tamil Nadu - Email me on Indeed: indeed.com/r/Arun-\\nElumalai/26575d617d50ea04 \\n \\n\\u2022 15 Months of Experience as a QA Tester in Software Testing (Mainframe) \\n \\n\\u2022 Experience in Automation, Functional, UI testing and Regression \\nTesting. \\n \\n\\n\\u2022 Involvement in preparation of Test scenarios, Test cases and executing \\nthe same. \\n \\n\\u2022 Defect reporting and tracking via Rational Quality Manager. \\n \\n\\u2022 Preparation of test closure reports. \\n \\nWORK EXPERIENCE \\n \\nQA Tester \\n \\nAccenture - \\n \\nNovember 2016 to March 2018 \\n \\n\\n\\u2022 Associate Software Engineer | Accenture Services Pvt Ltd | Nov 2016 to \\nMar 2018 \\n\\u2022 Domain: Financial Services - Payments Domain \\n\\u2022 Application: VisionPLUS \\nPROJECT PROFILE \\n \\nClient - First Data Corporation \\nRole - QA Tester \\nApplication - VisionPLUS \\n \\nDescription and Responsibilities \\n1. Have worked in functional releases and tested across clients in the \\nEMEA region. Performed \\nsystem integration testing for new clients that came into VisionPlus. \\n2. Automated manual scripts in Regression Testing and Executing the same \\nusing Selenium Web \\ndriver through Sauce Labs. \\n3. Performed UI Testing in First Apply and First Online \\n4. Tested various functionalities of credit card life cycle like account \\nboarding, embossing, \\naccount/card transfer, replacement and reissue of cards. \\n5. Tested manual and auto enrollment of offers, cashback offers. \\n \\nPERFORMANCE ACHIEVEMENTS \\n \\n\\u2022 Won The Rising Star Award for the year 2017 \\n \\nEDUCATION \\n \\nBachelor of Engineering in Automobile Engineering \\n \\nhttps://www.indeed.com/r/Arun-Elumalai/26575d617d50ea04?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n\\n\\f \\nSri Venkateswara College Of Engineering -  Chennai, Tamil Nadu \\n \\n2012 to 2016 \\n \\nSKILLS \\n \\nANSYS (Less than 1 year), CATIA (Less than 1 year), CREO (Less than 1 \\nyear), PARAMETRIC \\n(Less than 1 year), PYTHON (Less than 1 year), Selenium, Selenium \\nWebdriver, Testing, \\nFunctional Testing, Automation Testing, Regression Testing, Quality \\nAssurance \\n \\nADDITIONAL INFORMATION \\n \\nTECHNICAL SKILLS \\n \\n\\u2022 Languages - Python \\n \\n\\u2022 Software/Tools - Selenium, WAF, Sauce Labs, Jenkins, Creo parametric \\n2.0, Catia V6, Ansys \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume11.pdf\",\n",
            "                \"text\": \"Avin Sharma \\nSenior Associate Consultant - Infosys Limited \\n \\nHyderabad, Telangana - Email me on Indeed: indeed.com/r/Avin-\\nSharma/3ad8a8b57a172613 \\n \\nWORK EXPERIENCE \\n \\nSenior Associate Consultant \\n \\nInfosys Limited - \\n \\nJuly 2015 to Present \\n \\nWorked on Presales activities preparing Proposals, RFP's, preparing \\npresentations, budgets/ \\nquotations based \\non client requirements. \\n\\n\\u2022 Responsible for entire sales cycle from market research for prospective \\nclients to final \\nnegotiation & sales \\nclosure. \\n\\u2022 Worked on mapping commercials to increase the profits and resources \\nplanning/ utilization. \\n\\n\\u2022 Leading and coordinating a team of business analysts, developers and \\ntesters for execution \\nof multiple projects. \\n\\u2022 Developed strategies and generated business for the firm by building \\ncorporate relationships \\nwith client. \\n \\nSenior System Engineer \\n \\nInfosys Limited - \\n \\nAugust 2008 to April 2013 \\n \\nWorked as a quality analyst for client; performed system testing for \\ntheir customer web portal \\nhaving modules \\nlike Dashboard, Billing, Shop, Payments, Login etc. \\n\\n\\u2022 Led a team of three system engineers; identified competency gap and \\nconducted knowledge \\nsharing sessions \\nfor new team members. \\n\\n\\u2022 Communicated with the client team members on frequent basis for \\nunderstanding their issues \\nand bringing \\naction items to closure. \\n\\n\\u2022 Independently handled the responsibility for designing manual test \\ncases and executing them. \\n\\n\\u2022 Worked on client's merger Integration application as a Developer which \\ninvolved migration from \\nlower framework \\nto higher framework. \\n\\n\\u2022 Responsible for migrating the part of application developed in dot net \\nfrom lower version to \\nhigher version. \\n\\n\\u2022 Developed the internal portals on Dot Net platform for Infosys team \\nmembers. \\n\\n\\f\\u2022 Completed the assigned tasks as per the timelines and ensured that the \\ngiven tasks are \\ncompleted with compliance to the benchmarks. \\n \\nhttps://www.indeed.com/r/Avin-Sharma/3ad8a8b57a172613?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\u2022 Received STAR Certification for showcasing outstanding Soft Skills on \\n'Business & interpersonal \\ncommunication \\nfor Client delight. \\n \\nGuru Nanak Dev University \\n \\nAmritsar, Punjab - \\n \\nJuly 2004 to June 2008 \\n \\nEDUCATION \\n \\nGreat Lakes Institute of Management -  Chennai, Tamil Nadu \\n \\nApril 2014 to April 2015 \\n \\nSKILLS \\n \\nRequirement Analysis (Less than 1 year), Sales support (Less than 1 \\nyear), Test Planning (Less \\nthan 1 year) \\n \\nADDITIONAL INFORMATION \\n \\nSkills \\nBid management, Sales support, Requirement Analysis, Test Planning and \\nTest execution \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume8.pdf\",\n",
            "                \"text\": \"Ashalata Bisoyi \\nTransaction Processor - Oracle India Private Limited \\n \\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Ashalata-\\nBisoyi/cf02125911cfb5df \\n \\nTo secure a position an esteem organization with good working culture \\nthat will help my career \\nin the field of finance through my sincerity, hard works and skills. \\n \\nWORK EXPERIENCE \\n \\nTransaction Processor \\n \\nOracle India Private Limited - \\n \\nApril 2016 to Present \\n \\n2 Year of experience with Oracle India Private Limited in expense team. \\nMy work is auditing of \\nexpense reports of employees of all the countries, handling queries \\nthrough emails and calls also. \\nJOB DESCRIPTION \\n\\n\\u2022 Auditing of expense reports of the employees for all the countries and \\nworking on service portal \\n(Answering queries through email) \\n\\u2022 Handling the team in absence of seniors. \\n\\u2022 Working on Payment Rejections, export of expense reports to AP. \\n\\u2022 Take care of running Backlog, Having knowledge about travel advance. \\n \\nEDUCATION \\n \\nMaster of Finance and Control in MFC \\n \\nKhallikote Autonomous college -  Brahmapur, Orissa \\n \\n2015 \\n \\nBachelor in Commerce \\n \\nBerhampur university -  Brahmapur, Orissa \\n \\n2013 \\n \\nAccounting \\n \\nScience College, Hinjilicut -  Brahmapur, Orissa \\n \\n2010 \\n \\nEducation \\n \\nCouncil of Higher Secondary -  Orissa, IN \\n \\n2008 \\n \\nGovernment girls High school -  Hinjilikatu, Orissa \\n \\nhttps://www.indeed.com/r/Ashalata-Bisoyi/cf02125911cfb5df?isid=rex-\\ndownload&ikw=download-top&co=IN \\n\\n\\f \\n \\nBoard of Secondary Education -  Orissa, IN \\n \\nADDITIONAL INFORMATION \\n \\n\\u2022 Good analytical skills and flexible in work atmosphere. \\n\\u2022 Able to handle complex situation under process \\n\\u2022 Willingness to learn \\n\\u2022 Ability to meet deadlines \\n\\u2022 Every time accept the new challenges \\n \\nOTHER QUALIFICATIONS \\n\\u2022 DOEACC O LEVEL \\n\\u2022 M.S. OFFICE \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume10.pdf\",\n",
            "                \"text\": \"Asish Ratha \\nSubject matter Expert - Accenture \\n \\nChennai, Tamil Nadu - Email me on Indeed: indeed.com/r/Asish-\\nRatha/853988e0e0e236a3 \\n \\nWORK EXPERIENCE \\n \\nSubject matter Expert \\n \\nAccenture - \\n \\nMarch 2012 to Present \\n \\nSubject matter expert \\n \\nEDUCATION \\n \\nBerhampur university, Khallikote autonomous college -  Brahmapur, Orissa \\n \\nSKILLS \\n \\nInvoice (5 years), posting. (5 years), TRAINING (4 years) \\n \\nADDITIONAL INFORMATION \\n \\nSKILLS \\nInvoice processing, Team handling, new joiners training.sap \\nposting,vendor call attend and \\nresolve the issue,meet SLA tat,working with client tool. \\n \\nhttps://www.indeed.com/r/Asish-Ratha/853988e0e0e236a3?isid=rex-\\ndownload&ikw=download-top&co=IN \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume12.pdf\",\n",
            "                \"text\": \"Ayesha B \\nTeam member - Oracle \\n \\nBangalore, Karnataka - Email me on Indeed: indeed.com/r/Ayesha-\\nB/b2985be284dee3d6 \\n \\nSeeking a position to utilise my skills and abilities in the industry \\nthat offers professional growth \\nwhile being innovative, flexible and also to explore myself and utilise \\nmy potential to the growth \\nof the company. \\n \\nWORK EXPERIENCE \\n \\nTeam member \\n \\nOracle -  Bangalore, Karnataka - \\n \\nAugust 2012 to Present \\n \\nNature of duties: \\n \\n\\u2022 Developed and deployed Oracle forms in FLEXCUBE CORPORATE. \\n- I have developed Oracle Forms and deployed the same in application \\nserver, which is used as \\nfront end by the bank users. \\n\\u2022 Necessary coding changes of project are done in PL/SQL developer. \\n- Necessary Procedures and Functions are defined as part of the project. \\n\\u2022 Was part of Internal Unit Testing and Unit Testing. \\n- I have prepared Test cases for forms and tested the same and was part \\nof Internal Unit Testing. \\n\\u2022 Documentation. \\n- I have prepared Functional Specification, Desgin Specification and \\nProject specification \\ndocumentation. \\n\\u2022 Installed and worked on FLEXML. \\n\\u2022 Exported and Imported schemas. \\n \\nEDUCATION \\n \\nB.E. in CSE \\n \\nAtria Institute of Technology -  Bangalore, Karnataka \\n \\n2012 \\n \\nPre University Education -  Bangalore, Karnataka \\n \\n2008 \\n \\nC.B.S.E. \\n \\nSindhi High School -  Bangalore, Karnataka \\n \\n2006 \\n \\nhttps://www.indeed.com/r/Ayesha-B/b2985be284dee3d6?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\n\\fADDITIONAL INFORMATION \\n \\nSOFT SKILLS \\n \\n* Sincere and Hardworking in nature \\n* Highly Dedicated towards work \\n \\n* Efficient Individual and Team Player \\n* Goal Oriented & Self Motivated \\n \\nIT Literacy \\n\\n\\f\"\n",
            "            },\n",
            "            {\n",
            "                \"file_name\": \"Resume9.pdf\",\n",
            "                \"text\": \"Ashok Kunam \\nTeam Lead - Microsoft \\n \\n- Email me on Indeed: indeed.com/r/Ashok-Kunam/7aac8767aacf10a0 \\n \\n\\n\\u2022 Software Engineering professional with over 2.5 years of experience in \\ndevelopment, \\nenhancement, integration, implementation and maintenance of software \\napplications using \\nOracle \\n\\n\\u2022 Served some of the major clients such as Microsoft, Cisco, Oracle \\nEloqua, iTalent \\n\\n\\u2022 Knowledge of Operating Systems (OS) coding techniques, Internet \\nProtocol (IP), interfaces and \\nhardware subsystems, reading schematics and data sheets for components \\n\\n\\u2022 Expertise in Java, J2EE, hibernate, spring, PHP, Nifi, Angular JS, \\nJavaScript, JQuery, REST Web \\nservices, Jive, Lithium \\n\\n\\u2022 Successfully developed cloud based connectors using REST Web services \\nand JIVE custom tiles \\n& plugins \\n\\u2022 Proficient in end-to-end implementation of various projects; including \\ndesigning, development, \\ncoding, debugging, analyzing the logs & implementation of software \\napplications \\n\\n\\u2022 Proven skills of understanding business requirements and translating \\nthem into technical \\nspecifications \\n\\n\\u2022 Possess interpersonal, analytical and negotiation skills with proven \\ntrack record of utilizing a \\nprocess-oriented approach towards the accomplishment of cost, profit, \\nservice & organizational \\ngoals \\n \\nWORK EXPERIENCE \\n \\nTeam Lead \\n \\nMicrosoft - \\n \\nJuly 2017 to Present \\n \\n\\n\\u2022 Environment: Java 1.8, Spring MVC, Hibernate, REST API Calls, Power BI \\ndesktop tool, Lithium \\nBulk data API, SQL Server, Maven, Log 4j, Git, JIRA, Scrum \\n\\u2022 Role: Team Lead \\n\\u2022 Period: July '17 - Till Date \\n\\u2022 Description: Power BI deployed on Lithium Community, 7, 000 Power BI \\nusers can collaborate \\nand transfer the knowledge among them. On an average 150 k activity per \\nday in the community. \\nCloud Connector is required to fetch the all the activities from the \\ncommunity and perform data \\nmining and preserve into database as report generator format \\n\\u2022 Highlights: \\n\\u25e6 Provided support in the total software development life cycle of the \\nproject \\n\\u25e6 Implemented cloud connector to read the Lithium bulk data API, parse \\nthe data and inserted \\nit into SQL Server \\n\\n\\f\\u25e6 Generated views as per requirements & prepared Power BI reports \\n \\n\\u2022 Organization: iTalent Corporation \\n\\u2022 Title: MyiTalent \\n\\u2022 Client: iTalent \\n \\nhttps://www.indeed.com/r/Ashok-Kunam/7aac8767aacf10a0?isid=rex-\\ndownload&ikw=download-top&co=IN \\n \\n \\n\\n\\u2022 Environment: Java 1.8, PHP, REST API Calls, hibernate, Angular JS, CSS, \\nMySQL, Maven, Log \\n4j, Git, JIRA, Linux \\n\\u2022 Role: Team Lead \\n\\u2022 Period: July '17 - Till Date \\n\\u2022 Description: Instantly find and hire the right candidate for your job \\nopening. With MyiTalent, \\ncorporate human resource managers and hiring managers can now easily \\nmanage the hiring \\nprocess with their recruiters. Review and select from pre-screened \\ncandidates that match your \\nrequirements. Find and identify experts within your company. Browse \\nproject scope of work and \\ndetails, including budget information. Collaborate with your own personal \\nrecruiter at all times. \\nProvide feedback on candidates and projects \\n\\u2022 Highlights: \\n\\u25e6 Developed PHP Backend Services \\n\\u25e6 Implemented ZOHO integration for jobs and candidates using REST web \\nservices \\n\\u25e6 Created Cron Job to fetch candidates from ZOHO \\n \\n\\u2022 Organization: iTalent Corporation \\n\\u2022 Title: CSC \\n\\u2022 Client: Cisco \\n\\u2022 Environment: Java 1.8, Spring MVC, Hibernate, REST API Calls, Lithium \\nBulk data API, MySQL, \\nMaven, Log 4j, Git, JIRA, Scrum \\n\\u2022 Role: Team Lead \\n\\u2022 Period: July '17 - Till Date \\n\\u2022 Description: Migration from Drupal community to the Lithium Community \\nwhich is enhanced \\nwith custom cloud search using Attivio. \\n\\u2022 Highlights: \\n\\u25e6 Provided support in the total software development life cycle of the \\nproject \\n\\u25e6 Implemented cloud connector to read the Lithium bulk data API, parse \\nthe data and inserted \\nit into SQL Server \\n\\u25e6 Generated views as per requirements & prepared Power BI reports \\n \\nEDUCATION \\n \\nBachelor of Technology in ECE \\n \\nJawaharlal Nehru Technology University -  Kakinada, Andhra Pradesh \\n \\n2014 \\n \\nSKILLS \\n\\n\\f \\ndatabase (Less than 1 year), Git (Less than 1 year), Java (Less than 1 \\nyear), JIRA (Less than 1 \\nyear), life cycle (Less than 1 year) \\n \\nLINKS \\n \\nhttps://www.linkedin.com/in/ashok-kunam-85a845a8 \\n \\nhttps://www.linkedin.com/in/ashok-kunam-85a845a8 \\n \\n \\nADDITIONAL INFORMATION \\n \\nTechnical Skills \\n \\n\\u2022 Programming Languages: Java & J2EE, PHP, Apache Nifi, SOAP Web \\nServices, Rest Web Services, \\nJackson-2 \\n\\u2022 Framework's: Spring, Hibernate, Junit, Mockito \\n\\u2022 Database: MYSQL, SQL Server, Oracle \\n\\u2022 App/Web Servers: Tomcat, Apache \\n \\n\\u2022 Operating Systems: Windows, Linux \\n\\u2022 SKM: Lithium \\n\\u2022 Tools: Git, JIRA, Eclipse, putty \\n \\nKnowledge Purview \\n \\n\\u2022 Methodologies and Principles: \\n\\u25e6 Code Generation Tools \\n\\u25e6 Documentation Generation \\n\\u25e6 Inversion of Control \\n\\u25e6 Iterative Development \\n\\u25e6 Object Oriented Programming \\n\\u25e6 Deployment \\n\\u25e6 Agile \\n \\nProfessional Experience \\n \\nCore Competencies \\n \\n~ Requirement Gathering \\n~ Deployment & Support \\n~ Project Management \\n~ Software Development Life Cycle \\n~ Delivery Management \\n~ Quality Assurance \\n~ Application / Software Development \\n~ Liaison & Coordination \\n\\n\\f\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl90l5mYxhZ6",
        "colab_type": "text"
      },
      "source": [
        "In the following table the vectors formed by applying the first four steps on one resume and one job description are presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPo2rjxe6-RX",
        "colab_type": "code",
        "outputId": "8c6cc5ab-4083-4fe3-ac4a-1b03c40277c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "#the TF-IDF measure for one pair resume-job description\n",
        "j_d = documents[\"Candidatul_ideal##03-04-2020  18-06-16_SET\"]['job_description']['text']\n",
        "resume_1 = documents[\"Candidatul_ideal##03-04-2020  18-06-16_SET\"]['resumes'][0]['text']\n",
        "match_resume_job_description(j_d,resume_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>python</th>\n",
              "      <th>docker</th>\n",
              "      <th>hive</th>\n",
              "      <th>hadoop</th>\n",
              "      <th>sql</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Job Description</th>\n",
              "      <td>0.048797</td>\n",
              "      <td>0.048797</td>\n",
              "      <td>0.048797</td>\n",
              "      <td>0.048797</td>\n",
              "      <td>0.048797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Resume</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   python    docker      hive    hadoop       sql\n",
              "Job Description  0.048797  0.048797  0.048797  0.048797  0.048797\n",
              "Resume           0.000000  0.000000  0.000000  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD0CjUxzxNnB",
        "colab_type": "text"
      },
      "source": [
        ">The results obtained by applying the entire sollution can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vrqddot3F7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the ranking alghoritm is applied\n",
        "matches = {}\n",
        "\n",
        "for batch_key in documents.keys():\n",
        "  \n",
        "  batch = documents[batch_key]\n",
        "  job_description_name = batch['job_description']['file_name'] \n",
        "  job_description_text = batch['job_description']['text']\n",
        "\n",
        "  \n",
        "  resumes_scores = []\n",
        "\n",
        "  for resume_json in batch['resumes']:\n",
        "  \n",
        "    resume_name = resume_json['file_name']\n",
        "    resume_text = resume_json['text']\n",
        "\n",
        "    match = {}\n",
        "    score = match_and_calculate(job_description_text,\n",
        "                                resume_text)\n",
        "    \n",
        "    match['resume'] = resume_name\n",
        "    match['score'] = score\n",
        "\n",
        "    resumes_scores.append(match)\n",
        "\n",
        "  resumes_scores.sort(\n",
        "      key = lambda x : float(x['score']),\n",
        "      reverse = True\n",
        ")\n",
        "  matches[job_description_name] = resumes_scores\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnCvvaGS47Pw",
        "colab_type": "code",
        "outputId": "73d79298-da41-45bc-b2ac-b8e62a667a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(json.dumps(matches,indent = 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"Candidatul_ideal##03-04-2020  18-06-16.pdf\": [\n",
            "        {\n",
            "            \"resume\": \"Resume5.pdf\",\n",
            "            \"score\": 0.4765509220328154\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume3.pdf\",\n",
            "            \"score\": 0.33517574332792605\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume2.pdf\",\n",
            "            \"score\": 0.33517574332792605\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume1.pdf\",\n",
            "            \"score\": 0.33517574332792605\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume4.pdf\",\n",
            "            \"score\": 0.0\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume0.pdf\",\n",
            "            \"score\": 0.0\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume6.pdf\",\n",
            "            \"score\": 0.0\n",
            "        }\n",
            "    ],\n",
            "    \"We_are_looking_for_a_Big_Data_Engineer##03-04-2020  18-10-16.pdf\": [\n",
            "        {\n",
            "            \"resume\": \"Resume18.pdf\",\n",
            "            \"score\": 0.4494364165239822\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume17.pdf\",\n",
            "            \"score\": 0.44943641652398214\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume19.pdf\",\n",
            "            \"score\": 0.4494364165239821\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume16.pdf\",\n",
            "            \"score\": 0.44943641652398203\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume14.pdf\",\n",
            "            \"score\": 0.0\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume13.pdf\",\n",
            "            \"score\": 0.0\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume15.pdf\",\n",
            "            \"score\": 0.0\n",
            "        }\n",
            "    ],\n",
            "    \"Job_Description_3##03-04-2020  18-11-36.pdf\": [\n",
            "        {\n",
            "            \"resume\": \"Resume7.pdf\",\n",
            "            \"score\": 0.7092972666062738\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume12.pdf\",\n",
            "            \"score\": 0.30321606445038635\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume9.pdf\",\n",
            "            \"score\": 0.3032160644503863\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume11.pdf\",\n",
            "            \"score\": 0.0\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume8.pdf\",\n",
            "            \"score\": 0.0\n",
            "        },\n",
            "        {\n",
            "            \"resume\": \"Resume10.pdf\",\n",
            "            \"score\": 0.0\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxtnUJ83zaSw",
        "colab_type": "text"
      },
      "source": [
        "##5.Conclusions and future work\n",
        "\n",
        ">As mentioned in [8], we cannot train a model to rank resumes acording to their suitability for a specific job description due to a lack of ground truth, the only thing we can do is to look for aspects in the resumes are objectively suited for the job description. \n",
        "\n",
        ">In this paper, we presented our sollution for the task of ranking resumes acording to a specific job description,by preprocessing the text, creating text sets of two documents(one resume and one job description), calculating the TF-IDF metric, extracting entities and performing the cosine similarity on the formed vector.\n",
        "\n",
        ">The proposed sollution suffers from a series of drawbacks, first and most important, the system relies on predefined keywords to succesfully calculate the matching score. The sollutions for this problem are numerous, one proposed aproach is the training of a SVM statistical entity recognizer that detects if a specific word represents a skill(or a keyword) or not. The training of statistical models with multiple labels suffering from the ever-changing nature of jobs, new jobs that requieres new skills appearning and others are dissappearing, being necesary to retrain the models every time a new specific skill is presented. Another problem with the proposed system is the identical scores of resumes. As presented in the previous section, it is possible to have resumes with the exact same score, the main reason being the lack of keywords that can also be fixed by using a binary model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRQ9dnPYTi1s",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        ">[1] SHIVRATRI, Pooja, et al. Resume parsing and standardization. International Journal of Computer Sciences and Engineering, 2015, 3.3: 129-131.\n",
        "\n",
        ">[2] HAN, Hui, et al. Automatic document metadata extraction using support vector machines. In: 2003 Joint Conference on Digital Libraries, 2003. Proceedings. IEEE, 2003. p. 37-48.\n",
        "\n",
        ">[3] CHEN, Jiaze; GAO, Liangcai; TANG, Zhi. Information extraction from resume documents in pdf format. Electronic Imaging, 2016, 2016.17: 1-8.\n",
        "\n",
        ">[4] CHANG, Chih-Chung; LIN, Chih-Jen. LIBSVM: A library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2011, 2.3: 1-27.\n",
        "\n",
        ">[5] ÇELIK, Duygu, et al. Towards an information extraction system based on ontology to match resumes and jobs. In: 2013 IEEE 37th Annual Computer Software and Applications Conference Workshops. IEEE, 2013. p. 333-338.\n",
        "\n",
        ">[6] SHIVRATRI, Pooja, et al. Resume parsing and standardization. International Journal of Computer Sciences and Engineering, 2015, 3.3: 129-131.\n",
        "\n",
        ">[7] DAS, Papiya; PANDEY, Manjusha; RAUTARAY, Siddharth Swarup. A CV Parser Model using Entity Extraction Process and Big Data Tools. IJ Information Technology and Computer Science, 2018, 9: 21-31.\n",
        "\n",
        ">[8] BHATIA, Vedant, et al. End-to-End Resume Parsing and Finding Candidates for a Job Description using BERT. arXiv preprint arXiv:1910.03089, 2019.\n",
        "\n",
        ">[9] DEVLIN, Jacob, et al. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. \n",
        "\n",
        ">[10] SANYAL, Satyaki, et al. Resume Parser with Natural Language Processing. International Journal of Engineering Science, 2017, 4484.\n",
        "\n",
        ">[11] VIJAYARANI, S.; ILAMATHI, Ms J.; NITHYA, Ms. Preprocessing techniques for text mining-an overview. International Journal of Computer Science & Communication Networks, 2015, 5.1: 7-16.\n",
        "\n",
        ">[12] JAVED, Faizan, et al. Towards a job title classification system. arXiv preprint arXiv:1606.00917, 2016.\n",
        "\n",
        ">[13] KADHIM, Ammar Ismael. An Evaluation of Preprocessing Techniques for Text Classification. International Journal of Computer Science and Information Security (IJCSIS), 2018, 16.6."
      ]
    }
  ]
}